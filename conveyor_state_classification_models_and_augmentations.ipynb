{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "DiOgyPTmZpXn",
        "m8Sk29J5xUto",
        "0ilblm5OS98s",
        "wjpFXy0Pxgrn",
        "taARxUyjUabc",
        "92ViCdnwxkTe",
        "PovjjIKMxpHu",
        "cxmWkMVzVgyA",
        "rnJMc2UJ4JpX",
        "FPYr72yrVlcZ",
        "JDAIgKFsyE0c",
        "5jx3M3hKEVU7",
        "voIK-BHWyTiO",
        "hdJ-6aGhyX6I",
        "CrNMqN8O0w9h",
        "9zlrXeWP06XN",
        "FiI77dDL9NaW",
        "g6bk8TWsyjby",
        "ca6OeXwqytJp",
        "PFd62aQSyyDD",
        "resZbFb6y17H",
        "ehIaBuOfjBrm",
        "J4EcTkTtkzKx",
        "mFQ6uJQFtvrq",
        "SiROGmfIzn3Y",
        "TYa7km-jpF7z",
        "EpEYtIiopIv4",
        "hkxMJS5Xe9sT",
        "p2BBrBZHYd75",
        "C6WnkaAJEkqK",
        "NMECRZKVuPxt",
        "Gf1HJrPVfpDg",
        "5f38ga6r0YJk",
        "QuHrKjy917TX",
        "WwYv3YQcbpHu",
        "cz3Fzfhp7KGt",
        "3b4yoDqMYCAm",
        "fCe08FVTZDhT",
        "EkOncaha7Mp1",
        "SAktcAqNi5Ua",
        "ilySMcEQlbvb",
        "lqHsGUDbuXiZ",
        "AZjNXCEtxSd5",
        "BiStNrCppyKG",
        "yAwLi01YAiPR",
        "22ZHHeXn7QhB",
        "gDVc1ybIcgd-",
        "WIyKN57k7mF-",
        "W_386Y_DhgiS",
        "wnBsD3dw2WH8",
        "cbYqNTK0p79t"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Bachelor. Conveyor belt state classification using deep neural networks and data augmentation methods.\n",
        "\n",
        "Vilnius University \\\\\n",
        "Software Engineering \\\\\n",
        "Student Armantas Pik≈°rys"
      ],
      "metadata": {
        "id": "fjLzPOrAZZBu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import and Mount data"
      ],
      "metadata": {
        "id": "DiOgyPTmZpXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchvision"
      ],
      "metadata": {
        "id": "GaTmbfFBZXn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sktime\n",
        "!pip install pmdarima\n",
        "!pip install tsai"
      ],
      "metadata": {
        "id": "JrDGnlwLlCrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import sktime\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "import pmdarima as pm\n",
        "import seaborn as sns\n",
        "from sktime.forecasting.model_selection import temporal_train_test_split\n",
        "from sktime.forecasting.base import ForecastingHorizon\n",
        "from sktime.forecasting.sarimax import SARIMAX\n",
        "from sktime.forecasting.arima import AutoARIMA\n",
        "from sktime.forecasting.compose import (\n",
        "    EnsembleForecaster,\n",
        "    MultiplexForecaster,\n",
        "    TransformedTargetForecaster,\n",
        "    make_reduction,\n",
        ")\n",
        "from sktime.forecasting.model_evaluation import evaluate\n",
        "from sktime.forecasting.model_selection import (\n",
        "    ExpandingWindowSplitter,\n",
        "    ForecastingGridSearchCV,\n",
        "    SlidingWindowSplitter,\n",
        "    temporal_train_test_split,\n",
        ")\n",
        "from sktime.forecasting.exp_smoothing import ExponentialSmoothing\n",
        "from sktime.forecasting.naive import NaiveForecaster\n",
        "from sktime.forecasting.theta import ThetaForecaster\n",
        "from sktime.forecasting.trend import PolynomialTrendForecaster\n",
        "from sktime.performance_metrics.forecasting import  MeanAbsolutePercentageError, MeanSquaredError\n",
        "from sktime.transformations.series.detrend import Deseasonalizer, Detrender\n",
        "from sktime.transformations.panel.rocket import MiniRocket\n",
        "from sktime.utils.plotting import plot_series\n",
        "from sktime.forecasting.compose import TransformedTargetForecaster\n",
        "from sktime.forecasting.trend import PolynomialTrendForecaster\n",
        "from sktime.transformations.panel.tsfresh import TSFreshFeatureExtractor\n",
        "from sktime.forecasting.fbprophet import Prophet\n",
        "from sktime.forecasting.tbats import TBATS\n",
        "from sktime.forecasting.ets import AutoETS\n",
        "from sktime.performance_metrics.forecasting import  MeanAbsolutePercentageError, MeanSquaredError\n",
        "from sktime.utils.plotting import plot_series\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sktime.datasets import load_from_tsfile\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import warnings\n",
        "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
        "from statsmodels.graphics.tsaplots import plot_acf\n",
        "from statsmodels.graphics.tsaplots import plot_pacf"
      ],
      "metadata": {
        "id": "ixAXxygalG7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tsai.all import *\n",
        "from tsai.data.transforms import ToTensor\n",
        "import warnings\n",
        "\n",
        "computer_setup()"
      ],
      "metadata": {
        "id": "S-cguuaulHrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "qcciSWmhvGYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "print(gpu_devices)\n",
        "if gpu_devices:\n",
        "    print('Using GPU')\n",
        "    tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
        "else:\n",
        "    print('Using CPU')"
      ],
      "metadata": {
        "id": "sVovdmIxvJTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "v_WKYplCa-Ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "tccCHm-fkxNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import TimeDistributed, Conv1D, BatchNormalization, AveragePooling1D, Dropout, Flatten\n",
        "from keras.layers import LSTM, Dense\n",
        "from keras import layers\n",
        "from keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, BatchNormalization, TimeDistributed, Conv1D, MaxPooling1D, Flatten\n",
        "import tensorflow.keras as keras"
      ],
      "metadata": {
        "id": "Ke8Wwhv7u1iM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read data"
      ],
      "metadata": {
        "id": "m8Sk29J5xUto"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read weight data"
      ],
      "metadata": {
        "id": "KCakT4g4S2o9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the window size\n",
        "window_size = 800\n",
        "\n",
        "# Directory where your files are located\n",
        "data_dir = \"./drive/MyDrive/kursinis/WeightAndIncisionDataCleaned\""
      ],
      "metadata": {
        "id": "PZ1XMgnXVDPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize empty lists for signals and labels\n",
        "signals = []\n",
        "labels = []"
      ],
      "metadata": {
        "id": "fJQss9VSu4Pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read Weight Data\n",
        "# Iterate over files in the directory\n",
        "for filename in os.listdir(data_dir):\n",
        "    if filename.endswith(\".csv\"):\n",
        "        file_path = os.path.join(data_dir, filename)\n",
        "        if float(filename.split(\"_\")[0]) == 0:\n",
        "            weight_label = float(0.5)\n",
        "        else:\n",
        "            weight_label = float(filename.split(\"_\")[0])\n",
        "\n",
        "        # Load CSV data\n",
        "        data = pd.read_csv(file_path)\n",
        "\n",
        "        # Create signals using a sliding window\n",
        "        print(filename)\n",
        "        num_rows = len(data)\n",
        "        step_size = 800\n",
        "\n",
        "        for i in range(0, num_rows - window_size + 1, step_size):\n",
        "            window_data = data.iloc[i:i+window_size]\n",
        "            signals.append(window_data.values)\n",
        "            labels.append(weight_label)"
      ],
      "metadata": {
        "id": "Iqpdm7k0u5zE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert data"
      ],
      "metadata": {
        "id": "wjpFXy0Pxgrn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert data regular"
      ],
      "metadata": {
        "id": "23fv99xIUWbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the lists to NumPy arrays\n",
        "signals_array = np.array(signals)\n",
        "labels_array = np.array(labels)"
      ],
      "metadata": {
        "id": "UerwsPU2VYH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = LabelEncoder()\n",
        "labels_array = label_encoder.fit_transform(labels_array)"
      ],
      "metadata": {
        "id": "o8ZIfu1zqX1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_labels, counts = np.unique(labels_array, return_counts=True)\n",
        "print(\"Unique Labels:\", unique_labels)\n",
        "print(\"Counts:\", counts)\n",
        "print(signals_array.shape)"
      ],
      "metadata": {
        "id": "i-mcR1xqu-WS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Signal mean calculation"
      ],
      "metadata": {
        "id": "92ViCdnwxkTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "F2IqDAPmEyjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overall_mean_signal = np.mean(signals_array, axis=0)"
      ],
      "metadata": {
        "id": "GP3fo4i54VF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_signals_by_weight = []\n",
        "\n",
        "for weight in [0.5,1,2,3,5,6]:\n",
        "    mask = (labels_array == weight)\n",
        "    mean_signal_weight = np.mean(signals_array[mask], axis=0)\n",
        "    mean_signals_by_weight.append(mean_signal_weight)\n",
        "\n",
        "mean_signals_by_weight = np.array(mean_signals_by_weight)"
      ],
      "metadata": {
        "id": "XXxQpkoD3qHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting\n",
        "fig, axs = plt.subplots(1, 3, figsize=(13, 5), sharex=True)\n",
        "\n",
        "# Convert time axis to seconds\n",
        "time_seconds = np.arange(overall_mean_signal.shape[0]) / 400.0\n",
        "\n",
        "# Define colors for each line\n",
        "colors = ['C0', 'C1', 'C2', 'C3', 'C5', 'C5']\n",
        "\n",
        "# Plot overall mean signal for each dimension\n",
        "legend_entries = []\n",
        "\n",
        "for i in range(3):\n",
        "    # axs[i].plot(overall_mean_signal[:, i], label=f'Overall Mean - Dim {i + 1}')\n",
        "    axs[i].set_xlabel('Laikas, s')\n",
        "    axs[i].set_ylabel('Amplitudƒó, ADU')\n",
        "\n",
        "    # Add label \"a)\", \"b)\", or \"c)\" to each subplot\n",
        "    axs[i].text(0.01, 0.99, f'{chr(97 + i)})', transform=axs[i].transAxes, va='top', ha='left', fontsize=12)\n",
        "\n",
        "    # Plot mean signals by weight category for each dimension\n",
        "    for j, mean_signal_weight in enumerate(mean_signals_by_weight):\n",
        "        label = f'{[\"Pa≈æeistas\",1,2,3,5,\"Pa≈æeistas\"][j]}' if j == 0 else f'{[\"Pa≈æeistas\",\"0,5\",1,2,3,5][j]} kg apkrova'\n",
        "        # line = axs[i].plot(time_seconds, mean_signal_weight[:, i], label=label)[0]\n",
        "        line = axs[i].plot(time_seconds, mean_signal_weight[:, i], label=label, color=colors[j])[0]\n",
        "        # Collect legend entries only once\n",
        "        if i == 0:\n",
        "            legend_entries.append(line)\n",
        "\n",
        "# Adjust legend position\n",
        "fig.legend(legend_entries, labels=[line.get_label() for line in legend_entries], loc='upper left', bbox_to_anchor=(0.94, 0.92))\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 0.95, 0.95])  # Adjust layout to prevent clipping of titles\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fK232OEaFVl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split data into train, test, val"
      ],
      "metadata": {
        "id": "PovjjIKMxpHu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split and normalize regular"
      ],
      "metadata": {
        "id": "cxmWkMVzVgyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N = signals_array.shape[0]\n",
        "N_train = int(N*0.7)\n",
        "N_val = int(N*0.9)\n",
        "\n",
        "N_train, N_val, N"
      ],
      "metadata": {
        "id": "YwFhHt_MoHH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(seed=123)\n",
        "arr = np.arange(N)\n",
        "np.random.shuffle(arr)\n",
        "\n",
        "arr.shape"
      ],
      "metadata": {
        "id": "me1fiMEsptEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mu = np.mean(signals_array[arr[:N_train]])\n",
        "va = np.std(signals_array[arr[:N_train]])"
      ],
      "metadata": {
        "id": "atFOOjCKvziI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mu)\n",
        "print(va)"
      ],
      "metadata": {
        "id": "q4QxUP3Fv0V-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = (signals_array[arr[:N_train]] - mu)/va\n",
        "x_val   = (signals_array[arr[N_train:N_val]] - mu)/va\n",
        "x_test  = (signals_array[arr[N_val:]] - mu)/va\n",
        "\n",
        "y_train = labels_array[arr[:N_train]]\n",
        "y_val   = labels_array[arr[N_train:N_val]]\n",
        "y_test  = labels_array[arr[N_val:]]\n",
        "\n",
        "n_classes = len(np.unique(y_train))\n",
        "\n",
        "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, n_classes"
      ],
      "metadata": {
        "id": "_MCcI9AJoOrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.mean(x_train))\n",
        "print(np.std(x_train))\n",
        "print(np.mean(x_val))\n",
        "print(np.std(x_val))\n",
        "print(np.mean(x_test))\n",
        "print(np.std(x_test))"
      ],
      "metadata": {
        "id": "zP0EZP8zv9cb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "class_weights = compute_class_weight(\n",
        "                                        class_weight = \"balanced\",\n",
        "                                        classes = np.unique(y_train),\n",
        "                                        y = y_train\n",
        "                                    )\n",
        "class_weights = dict(zip(np.unique(y_train), class_weights))\n",
        "class_weights"
      ],
      "metadata": {
        "id": "OcFT6Cv1B-YT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split and normalize TSAI lib"
      ],
      "metadata": {
        "id": "rnJMc2UJ4JpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## train, test, validation splits\n",
        "splits = get_splits(labels_array,\n",
        "                    n_splits=1,\n",
        "                    valid_size=0.2,\n",
        "                    test_size=0.1,\n",
        "                    shuffle=True,\n",
        "                    balance=False,\n",
        "                    stratify=True,\n",
        "                    random_state=43,\n",
        "                    show_plot=True,\n",
        "                    verbose=True)\n",
        "splits"
      ],
      "metadata": {
        "id": "m6J9mKe04Pgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract indices from the splits\n",
        "train_indices = splits[0]\n",
        "val_indices = splits[1]\n",
        "test_indices = splits[2]"
      ],
      "metadata": {
        "id": "trja8QpH4bp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute mean and standard deviation over the training set\n",
        "mu = np.mean(signals_array[train_indices])\n",
        "va = np.std(signals_array[train_indices])"
      ],
      "metadata": {
        "id": "Eigdndtx4e__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mu)\n",
        "print(va)"
      ],
      "metadata": {
        "id": "IR7fe-Gq4hYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize each split using the statistics computed from the training set\n",
        "x_train = (signals_array[train_indices] - mu) / va\n",
        "x_val = (signals_array[val_indices] - mu) / va\n",
        "x_test = (signals_array[test_indices] - mu) / va\n",
        "\n",
        "# Assign labels to each split\n",
        "y_train = labels_array[train_indices]\n",
        "y_val = labels_array[val_indices]\n",
        "y_test = labels_array[test_indices]\n",
        "\n",
        "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape"
      ],
      "metadata": {
        "id": "F_aOhuwo4lL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = signals_array[train_indices]\n",
        "x_val = signals_array[val_indices]\n",
        "x_test = signals_array[test_indices]\n",
        "\n",
        "y_train = labels_array[train_indices]\n",
        "y_val = labels_array[val_indices]\n",
        "y_test = labels_array[test_indices]\n",
        "\n",
        "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape"
      ],
      "metadata": {
        "id": "dGDdUucMpbdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.mean(x_train))\n",
        "print(np.std(x_train))\n",
        "print(np.mean(x_val))\n",
        "print(np.std(x_val))\n",
        "print(np.mean(x_test))\n",
        "print(np.std(x_test))"
      ],
      "metadata": {
        "id": "sJLLB5cR4tsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "class_weights = compute_class_weight(\n",
        "                                        class_weight = \"balanced\",\n",
        "                                        classes = np.unique(y_train),\n",
        "                                        y = y_train\n",
        "                                    )\n",
        "class_weights = dict(zip(np.unique(y_train), class_weights))\n",
        "class_weights"
      ],
      "metadata": {
        "id": "10oBHhMI6GVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data augmentations"
      ],
      "metadata": {
        "id": "ctn9DaDQyO0j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data augmentation methods"
      ],
      "metadata": {
        "id": "5jx3M3hKEVU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to add Laplace noise to a single signal\n",
        "def add_laplace_noise(signal, scale=0.01):\n",
        "    noise = np.random.laplace(scale=scale, size=signal.shape)\n",
        "    return signal + noise"
      ],
      "metadata": {
        "id": "yjDSFzlI3ma6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_laplace_noise_to_array(signal_array_values, signal_array_targets, scale=0.01, augmentation_multiplier=1):\n",
        "    combined_array = signal_array_values\n",
        "    combined_targets = signal_array_targets\n",
        "\n",
        "    for i in range(augmentation_multiplier):\n",
        "        noisy_array = signal_array_values.copy()\n",
        "\n",
        "        # Apply Laplace noise to each signal in the array\n",
        "        for i in range(noisy_array.shape[0]):\n",
        "            for j in range(noisy_array.shape[2]):\n",
        "                noisy_array[i, :, j] = add_laplace_noise(noisy_array[i, :, j])\n",
        "\n",
        "        # Append the Laplace-noised signals to x_train\n",
        "        combined_array = np.vstack([combined_array, noisy_array])\n",
        "\n",
        "        # Update labels accordingly\n",
        "        targets_original = signal_array_targets.copy()\n",
        "        combined_targets = np.concatenate([combined_targets, targets_original])\n",
        "\n",
        "    # Shuffle the combined data (regular + noised)\n",
        "    shuffle_indices = np.random.permutation(combined_array.shape[0])\n",
        "    combined_array = combined_array[shuffle_indices, :]\n",
        "    combined_targets = combined_targets[shuffle_indices]\n",
        "\n",
        "    return combined_array, combined_targets"
      ],
      "metadata": {
        "id": "X1d9qVLt3ly0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to add drift augmentation to a single signal\n",
        "def add_drift(signal, scale=0.01):\n",
        "    num_points = signal.shape[0]\n",
        "    time = np.arange(num_points)\n",
        "    drift = np.random.normal(scale=scale, size=num_points)\n",
        "    return signal + drift"
      ],
      "metadata": {
        "id": "UU_kI5SqBe_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_drift_to_array(signal_array_values, signal_array_targets, scale=0.01, augmentation_multiplier=1):\n",
        "    combined_array = signal_array_values\n",
        "    combined_targets = signal_array_targets\n",
        "\n",
        "    for i in range(augmentation_multiplier):\n",
        "        drifted_array = signal_array_values.copy()\n",
        "\n",
        "        # Apply drift augmentation to each signal in the array\n",
        "        for i in range(drifted_array.shape[0]):\n",
        "            for j in range(drifted_array.shape[2]):\n",
        "                drifted_array[i, :, j] = add_drift(drifted_array[i, :, j], scale=scale)\n",
        "\n",
        "        # Append the drifted signals to x_train\n",
        "        combined_array = np.vstack([combined_array, drifted_array])\n",
        "\n",
        "        # Update labels accordingly\n",
        "        targets_original = signal_array_targets.copy()\n",
        "        combined_targets = np.concatenate([combined_targets, targets_original])\n",
        "\n",
        "    # Shuffle the combined data (regular + drifted)\n",
        "    shuffle_indices = np.random.permutation(combined_array.shape[0])\n",
        "    combined_array = combined_array[shuffle_indices, :]\n",
        "    combined_targets = combined_targets[shuffle_indices]\n",
        "\n",
        "    return combined_array, combined_targets"
      ],
      "metadata": {
        "id": "_OonP2bPBguH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_uniform_noise(signal, scale=0.01):\n",
        "    noise = np.random.uniform(low=-scale, high=scale, size=signal.shape)\n",
        "    return signal + noise"
      ],
      "metadata": {
        "id": "rMmmT6vusOPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_uniform_noise_to_array(signal_array_values, signal_array_targets, scale=0.01, augmentation_multiplier=1):\n",
        "    combined_array = signal_array_values\n",
        "    combined_targets = signal_array_targets\n",
        "\n",
        "    for i in range(augmentation_multiplier):\n",
        "        uniformed_array = signal_array_values.copy()\n",
        "\n",
        "        # Apply drift augmentation to each signal in the array\n",
        "        for i in range(uniformed_array.shape[0]):\n",
        "            uniformed_array[i] = add_uniform_noise(uniformed_array[i], scale=scale)\n",
        "\n",
        "        # Append the drifted signals to x_train\n",
        "        combined_array = np.vstack([combined_array, uniformed_array])\n",
        "\n",
        "        # Update labels accordingly\n",
        "        targets_original = signal_array_targets.copy()\n",
        "        combined_targets = np.concatenate([combined_targets, targets_original])\n",
        "\n",
        "    # Shuffle the combined data (regular + drifted)\n",
        "    shuffle_indices = np.random.permutation(combined_array.shape[0])\n",
        "    combined_array = combined_array[shuffle_indices, :]\n",
        "    combined_targets = combined_targets[shuffle_indices]\n",
        "\n",
        "    return combined_array, combined_targets"
      ],
      "metadata": {
        "id": "axycL8jQry4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to add combined Laplace noise and drift augmentation to a single signal\n",
        "def add_laplace_and_drift_augmentation(signal, laplace_scale=0.01, drift_scale=0.01):\n",
        "    # Apply Laplace noise\n",
        "    signal_with_laplace = add_laplace_noise(signal, scale=laplace_scale)\n",
        "\n",
        "    # Apply drift augmentation to the signal with Laplace noise\n",
        "    signal_with_combined_augmentation = add_drift(signal_with_laplace, scale=drift_scale)\n",
        "\n",
        "    return signal_with_combined_augmentation"
      ],
      "metadata": {
        "id": "0LuIcsq9X1pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_laplace_and_drift_augmentation_to_array(signal_array_values, signal_array_targets, laplace_scale=0.01, drift_scale=0.01, augmentation_multiplier=1):\n",
        "    combined_array = signal_array_values\n",
        "    combined_targets = signal_array_targets\n",
        "\n",
        "    for _ in range(augmentation_multiplier):\n",
        "        augmented_array = signal_array_values.copy()\n",
        "\n",
        "        # Apply combined augmentation to each signal in the array\n",
        "        for i in range(augmented_array.shape[0]):\n",
        "            for j in range(augmented_array.shape[2]):\n",
        "                augmented_array[i, :, j] = add_laplace_and_drift_augmentation(augmented_array[i, :, j], laplace_scale=laplace_scale, drift_scale=drift_scale)\n",
        "\n",
        "        # Append the augmented signals to x_train\n",
        "        combined_array = np.vstack([combined_array, augmented_array])\n",
        "\n",
        "        # Update labels accordingly\n",
        "        targets_original = signal_array_targets.copy()\n",
        "        combined_targets = np.concatenate([combined_targets, targets_original])\n",
        "\n",
        "    # Shuffle the combined data (regular + augmented)\n",
        "    shuffle_indices = np.random.permutation(combined_array.shape[0])\n",
        "    combined_array = combined_array[shuffle_indices, :]\n",
        "    combined_targets = combined_targets[shuffle_indices]\n",
        "\n",
        "    return combined_array, combined_targets"
      ],
      "metadata": {
        "id": "uNDzA1CPX9y2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to add combined Laplace noise and uniform noise augmentation to a single signal\n",
        "def add_laplace_and_uniform_augmentation(signal, laplace_scale=0.01, uniform_scale=0.01):\n",
        "    # Apply Laplace noise\n",
        "    signal_with_laplace = add_laplace_noise(signal, scale=laplace_scale)\n",
        "\n",
        "    # Apply uniform noise augmentation to the signal with Laplace noise\n",
        "    signal_with_combined_augmentation = add_uniform_noise(signal_with_laplace, scale=uniform_scale)\n",
        "\n",
        "    return signal_with_combined_augmentation"
      ],
      "metadata": {
        "id": "6KNIwOrAoE6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_laplace_and_uniform_augmentation_to_array(signal_array_values, signal_array_targets, laplace_scale=0.01, uniform_scale=0.01, augmentation_multiplier=1):\n",
        "    combined_array = signal_array_values\n",
        "    combined_targets = signal_array_targets\n",
        "\n",
        "    for _ in range(augmentation_multiplier):\n",
        "        augmented_array = signal_array_values.copy()\n",
        "\n",
        "        # Apply drift augmentation to each signal in the array\n",
        "        for i in range(augmented_array.shape[0]):\n",
        "            augmented_array[i] = add_laplace_and_uniform_augmentation(augmented_array[i], laplace_scale=laplace_scale, uniform_scale=uniform_scale)\n",
        "\n",
        "        # Append the augmented signals to x_train\n",
        "        combined_array = np.vstack([combined_array, augmented_array])\n",
        "\n",
        "        # Update labels accordingly\n",
        "        targets_original = signal_array_targets.copy()\n",
        "        combined_targets = np.concatenate([combined_targets, targets_original])\n",
        "\n",
        "    # Shuffle the combined data (regular + augmented)\n",
        "    shuffle_indices = np.random.permutation(combined_array.shape[0])\n",
        "    combined_array = combined_array[shuffle_indices, :]\n",
        "    combined_targets = combined_targets[shuffle_indices]\n",
        "\n",
        "    return combined_array, combined_targets"
      ],
      "metadata": {
        "id": "QkbZf465oVe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import permutations\n",
        "\n",
        "def all_channel_permutations_augmentation(data, labels):\n",
        "    num_samples, width, num_channels = data.shape\n",
        "\n",
        "    # Generate all possible channel permutations\n",
        "    all_permutations = list(permutations(range(num_channels)))\n",
        "    print(all_permutations)\n",
        "\n",
        "    # Create arrays to store the augmented data and labels\n",
        "    augmented_data = np.zeros((num_samples * len(all_permutations), width, num_channels))\n",
        "    augmented_labels = np.zeros((num_samples * len(all_permutations),))\n",
        "\n",
        "    # Iterate over each sample\n",
        "    for i in range(num_samples):\n",
        "        # Get the original channels and label for the current sample\n",
        "        original_channels = data[i, :, :]\n",
        "        original_label = labels[i]\n",
        "\n",
        "        # Create all permutations for the current sample\n",
        "        permuted_samples = [original_channels[:, perm] for perm in all_permutations]\n",
        "\n",
        "        # Concatenate the permuted samples to the augmented data\n",
        "        augmented_data[i * len(all_permutations): (i + 1) * len(all_permutations), :, :] = permuted_samples\n",
        "\n",
        "        # Set the same label for all augmented samples\n",
        "        augmented_labels[i * len(all_permutations): (i + 1) * len(all_permutations)] = original_label\n",
        "\n",
        "    # Shuffle the augmented data and labels together\n",
        "    shuffle_indices = np.random.permutation(len(augmented_labels))\n",
        "    augmented_data = augmented_data[shuffle_indices, :, :]\n",
        "    augmented_labels = augmented_labels[shuffle_indices]\n",
        "\n",
        "    return augmented_data, augmented_labels"
      ],
      "metadata": {
        "id": "ViqYd57G1uBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def multiply_and_reshuffle(train_values, train_labels, multiplier=29):\n",
        "\n",
        "    # Check if the multiplier is a positive integer\n",
        "    if not isinstance(multiplier, int) or multiplier <= 0:\n",
        "        raise ValueError(\"Multiplier should be a positive integer.\")\n",
        "\n",
        "    # Initialize arrays to store multiplied and reshuffled data\n",
        "    multiplied_and_shuffled_values = train_values.copy()\n",
        "    multiplied_and_shuffled_labels = train_labels.copy()\n",
        "\n",
        "    for _ in range(multiplier - 1):\n",
        "        # Multiply the data and labels\n",
        "        multiplied_and_shuffled_values = np.vstack([multiplied_and_shuffled_values, train_values])\n",
        "        multiplied_and_shuffled_labels = np.concatenate([multiplied_and_shuffled_labels, train_labels])\n",
        "\n",
        "    # Shuffle the combined data\n",
        "    shuffle_indices = np.random.permutation(multiplied_and_shuffled_values.shape[0])\n",
        "    multiplied_and_shuffled_values = multiplied_and_shuffled_values[shuffle_indices, :]\n",
        "    multiplied_and_shuffled_labels = multiplied_and_shuffled_labels[shuffle_indices]\n",
        "\n",
        "    return multiplied_and_shuffled_values, multiplied_and_shuffled_labels\n"
      ],
      "metadata": {
        "id": "lhvWnVCG4pG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def magnitude_warp(x, sigma=0.2, knot=4):\n",
        "    from scipy.interpolate import CubicSpline\n",
        "    orig_steps = np.arange(x.shape[1])\n",
        "\n",
        "    random_warps = np.random.normal(loc=1.0, scale=sigma, size=(x.shape[0], knot+2, x.shape[2]))\n",
        "    warp_steps = (np.ones((x.shape[2],1))*(np.linspace(0, x.shape[1]-1., num=knot+2))).T\n",
        "    ret = np.zeros_like(x)\n",
        "    for i, pat in enumerate(x):\n",
        "        warper = np.array([CubicSpline(warp_steps[:,dim], random_warps[i,:,dim])(orig_steps) for dim in range(x.shape[2])]).T\n",
        "        ret[i] = pat * warper\n",
        "\n",
        "    return ret"
      ],
      "metadata": {
        "id": "LikfSnDZSgNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.interpolate import CubicSpline\n",
        "\n",
        "def random_curve_generator(seq_len, dimensions, num_control_points=4):\n",
        "    # Generate random points to construct the spline curve\n",
        "    control_points = np.random.rand(num_control_points, dimensions) * 2 - 1  # Points in the range [-1, 1]\n",
        "    # Ensure first and last points are zeros, so the output curve starts and ends at the original magnitude\n",
        "    control_points[0], control_points[-1] = np.zeros(dimensions), np.zeros(dimensions)\n",
        "    # Generate a sequence of X values (time) to interpolate\n",
        "    x = np.linspace(0, 1, num_control_points)\n",
        "    # Create a sequence of points to evaluate the spline\n",
        "    x_eval = np.linspace(0, 1, seq_len)\n",
        "    # Perform cubic spline interpolation\n",
        "    cs = CubicSpline(x, control_points, axis=0)  # Interpolates along each dimension independently\n",
        "    warp_curve = cs(x_eval)\n",
        "    return warp_curve\n",
        "\n",
        "def magnitude_warping(time_series, sigma=0.2):\n",
        "    size, seq_len, dimensions = time_series.shape\n",
        "    warped_time_series = np.empty_like(time_series)\n",
        "    for i in range(size):\n",
        "        # Obtain a random curve\n",
        "        random_curve = random_curve_generator(seq_len, dimensions)\n",
        "        # Perturb the time series using random curve\n",
        "        warped_time_series[i] = time_series[i] * (1 + sigma * random_curve)\n",
        "    return warped_time_series"
      ],
      "metadata": {
        "id": "QEBWr91vVB0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_magnitude_warping_to_array(signal_array_values, signal_array_targets, sigma=0.2, augmentation_multiplier=1):\n",
        "    combined_array = signal_array_values\n",
        "    combined_targets = signal_array_targets\n",
        "\n",
        "    for i in range(augmentation_multiplier):\n",
        "        warped_array = signal_array_values.copy()\n",
        "\n",
        "        warped_array = magnitude_warping(warped_array, sigma)\n",
        "\n",
        "        # Append the drifted signals to x_train\n",
        "        combined_array = np.vstack([combined_array, warped_array])\n",
        "\n",
        "        # Update labels accordingly\n",
        "        targets_original = signal_array_targets.copy()\n",
        "        combined_targets = np.concatenate([combined_targets, targets_original])\n",
        "\n",
        "    # Shuffle the combined data (regular + drifted)\n",
        "    shuffle_indices = np.random.permutation(combined_array.shape[0])\n",
        "    combined_array = combined_array[shuffle_indices, :]\n",
        "    combined_targets = combined_targets[shuffle_indices]\n",
        "\n",
        "    return combined_array, combined_targets"
      ],
      "metadata": {
        "id": "-DHaZZxBWmLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Applying data augmentations"
      ],
      "metadata": {
        "id": "voIK-BHWyTiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# x_train_noisy = x_train.copy()\n",
        "\n",
        "# # Apply combined augmentation to each signal in the array\n",
        "# for i in range(x_train.shape[0]):\n",
        "#     for j in range(x_train.shape[2]):\n",
        "#         x_train_noisy[i, :, j] = add_laplace_and_drift_augmentation(x_train[i, :, j], laplace_scale=0.01, drift_scale=0.01)"
      ],
      "metadata": {
        "id": "zBLFE1OdYb3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x_train_noisy, y_train_noisy = add_laplace_and_drift_augmentation_to_array(x_train, y_train, 0.01, 0.01, 62)\n",
        "\n",
        "# x_train_noisy.shape, y_train_noisy.shape"
      ],
      "metadata": {
        "id": "tdgfPTyKYS9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x_train_noisy = x_train.copy()\n",
        "\n",
        "# # Apply combined augmentation to each signal in the array\n",
        "# for i in range(x_train.shape[0]):\n",
        "#     x_train_noisy[i] = add_laplace_and_drift_augmentation(x_train_noisy[i], laplace_scale=0.01, uniform_scale=0.01)"
      ],
      "metadata": {
        "id": "ibAKzuj6o3yF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x_train_noisy, y_train_noisy = add_laplace_and_uniform_augmentation_to_array(x_train, y_train, 0.01, 0.01, 62)\n",
        "\n",
        "# x_train_noisy.shape, y_train_noisy.shape"
      ],
      "metadata": {
        "id": "PBwZwyTZovzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x_train_noisy, y_train_noisy = add_magnitude_warping_to_array(x_train, y_train, 0.2, 30)\n",
        "\n",
        "# x_train_noisy.shape, y_train_noisy.shape"
      ],
      "metadata": {
        "id": "RlYeZSxpXddH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x_train_noisy, y_train_noisy = add_uniform_noise_to_array(x_train, y_train, 0.02, 62)\n",
        "\n",
        "# x_train_noisy.shape, y_train_noisy.shape"
      ],
      "metadata": {
        "id": "k5TlgE0nssuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x_train_noisy, y_train_noisy = all_channel_permutations_augmentation(x_train, y_train)\n",
        "\n",
        "# x_train_noisy.shape, y_train_noisy.shape"
      ],
      "metadata": {
        "id": "IDz2PRdp1vD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_noisy, y_train_noisy = add_laplace_noise_to_array(x_train, y_train, 0.01, 24)\n",
        "\n",
        "x_train_noisy.shape, y_train_noisy.shape"
      ],
      "metadata": {
        "id": "xUhO0IdyAjRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test_noisy, y_test_noisy = add_laplace_noise_to_array(x_test, y_test, 0.05, 100)\n",
        "\n",
        "x_test_noisy.shape, y_test_noisy.shape"
      ],
      "metadata": {
        "id": "z3vQWLoj6U7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Example usage\n",
        "# x_train_noisy, y_train_noisy = add_drift_to_array(x_train, y_train, scale=0.01, augmentation_multiplier=40)\n",
        "\n",
        "# x_train_noisy.shape, y_train_noisy.shape"
      ],
      "metadata": {
        "id": "gYXo6IJrBrAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x_test, y_test = add_laplace_noise_to_array(x_test, y_test, 0.02, 10)\n",
        "\n",
        "# x_test.shape, y_test.shape"
      ],
      "metadata": {
        "id": "2CENd1yuAnRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x_train_noisy, y_train_noisy = add_drift_to_array(x_train, y_train, 0.05, 100)\n",
        "\n",
        "# x_train_noisy.shape, y_train_noisy.shape"
      ],
      "metadata": {
        "id": "x39WxsLkDUOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x_val, y_val = add_laplace_noise_to_array(x_val, y_val, 0.01, 31)\n",
        "\n",
        "# x_val.shape, y_val.shape"
      ],
      "metadata": {
        "id": "FjgHYnBHAngc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x_val, y_val = add_drift_to_array(x_val, y_val, 0.02, 10)\n",
        "\n",
        "# x_val.shape, y_val.shape"
      ],
      "metadata": {
        "id": "70g-mzVNDUnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function\n",
        "x_train_noisy, y_train_noisy = multiply_and_reshuffle(x_train, y_train, multiplier=1)\n",
        "\n",
        "x_train_noisy.shape, y_train_noisy.shape"
      ],
      "metadata": {
        "id": "exMP3mQ_4s5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x_test_noisy, y_test_noisy = add_drift_to_array(x_test, y_test, 0.05, 100)\n",
        "\n",
        "# x_test_noisy.shape, y_test_noisy.shape"
      ],
      "metadata": {
        "id": "sqNlfFgONWF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x_train_noisy, y_train_noisy = multiply_and_reshuffle(x_train, y_train, multiplier=130)\n",
        "\n",
        "# x_train_noisy.shape, y_train_noisy.shape"
      ],
      "metadata": {
        "id": "XdfBeY5R1srN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x_train_noisy, y_train_noisy = multiply_and_reshuffle(x_train_noisy, y_train_noisy, multiplier=5)\n",
        "\n",
        "# x_train_noisy.shape, y_train_noisy.shape"
      ],
      "metadata": {
        "id": "NIiShmS_mcfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Show train data with augmentation"
      ],
      "metadata": {
        "id": "hdJ-6aGhyX6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set plot style to ggplot\n",
        "original = x_train[15]\n",
        "# Define time axis\n",
        "time_interval = 1 / 400  # Time interval in seconds per time point\n",
        "time_axis = np.arange(window_size) * time_interval  # Convert time axis to seconds\n",
        "\n",
        "# Plotting original and generated data side by side horizontally\n",
        "fig, axs = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Plot original data\n",
        "id1 = 4\n",
        "axs[0].plot(time_axis,original[:, 0], label='Sensor 1')\n",
        "axs[0].plot(time_axis, original[:, 1], label='Sensor 2')\n",
        "axs[0].plot(time_axis, original[:, 2], label='Sensor 3')\n",
        "axs[0].text(0.01, 0.99, f'{chr(97)})', transform=axs[0].transAxes, va='top', ha='left', fontsize=12)\n",
        "axs[0].set_xlabel('Time (s)')\n",
        "axs[0].set_ylabel('Amplitude Normalized (ADU)')\n",
        "axs[0].legend()\n",
        "\n",
        "# Plot generated data\n",
        "id = 1\n",
        "axs[1].plot(time_axis, original[:, 1], label='Sensor 1')\n",
        "axs[1].plot(time_axis, original[:, 2], label='Sensor 2')\n",
        "axs[1].plot(time_axis, original[:, 0], label='Sensor 3')\n",
        "axs[1].text(0.01, 0.99, f'{chr(98)})', transform=axs[1].transAxes, va='top', ha='left', fontsize=12)\n",
        "axs[1].set_xlabel('Time (s)')\n",
        "axs[1].set_ylabel('Amplitude Normalized (ADU)')\n",
        "axs[1].legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M8dszHY8iILb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have signals_array containing your original signals\n",
        "original = x_train[15]\n",
        "\n",
        "# Generate Laplace noise\n",
        "laplace_noise_3 = add_laplace_noise(original, 0.01)\n",
        "laplace_noise_2 = add_laplace_noise(original, 0.02)\n",
        "laplace_noise_1 = add_laplace_noise(original, 0.05)\n",
        "\n",
        "# Convert time axis to seconds\n",
        "time_seconds = np.arange(signals_array[0].shape[0]) / 400.0\n",
        "\n",
        "# Plot all channels in a single plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.plot(time_seconds, original[:, 0], label='Original', color='blue')\n",
        "plt.plot(time_seconds, original[:, 1], color='blue')\n",
        "plt.plot(time_seconds, original[:, 2], color='blue')\n",
        "\n",
        "\n",
        "plt.plot(time_seconds, laplace_noise_3[:, 0], label='Laplace std/100', color='green')\n",
        "plt.plot(time_seconds, laplace_noise_3[:, 1], color='green')\n",
        "plt.plot(time_seconds, laplace_noise_3[:, 2], color='green')\n",
        "\n",
        "\n",
        "plt.plot(time_seconds, laplace_noise_2[:, 0], label='Laplace std/50', color='red')\n",
        "plt.plot(time_seconds, laplace_noise_2[:, 1], color='red')\n",
        "plt.plot(time_seconds, laplace_noise_2[:, 2], color='red')\n",
        "\n",
        "plt.plot(time_seconds, laplace_noise_1[:, 0], label='Laplace std/20', color='orange')\n",
        "plt.plot(time_seconds, laplace_noise_1[:, 1], color='orange')\n",
        "plt.plot(time_seconds, laplace_noise_1[:, 2], color='orange')\n",
        "\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('Amplitude Normalized (ADU)')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IKVTQJoCOXK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have signals_array containing your original signals\n",
        "original = x_train[15]\n",
        "\n",
        "# Generate Laplace noise\n",
        "warped = magnitude_warping(x_train[:20], 0.2)\n",
        "\n",
        "# Convert time axis to seconds\n",
        "time_seconds = np.arange(signals_array[0].shape[0]) / 400.0\n",
        "\n",
        "# Plot all channels in a single plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.plot(time_seconds, original[:, 0], label='Original', color='blue')\n",
        "plt.plot(time_seconds, original[:, 1], color='blue')\n",
        "plt.plot(time_seconds, original[:, 2], color='blue')\n",
        "\n",
        "\n",
        "plt.plot(time_seconds, warped[15][:, 0], label='Magnitude warping', color='green')\n",
        "plt.plot(time_seconds, warped[15][:, 1], color='green')\n",
        "plt.plot(time_seconds, warped[15][:, 2], color='green')\n",
        "\n",
        "\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('Amplitude Normalized (ADU)')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Al3FOPC8R6rN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models"
      ],
      "metadata": {
        "id": "WZxIk3Jvyd1B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Resnet"
      ],
      "metadata": {
        "id": "VT0W4wR2ygGZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Resnet helpers"
      ],
      "metadata": {
        "id": "CrNMqN8O0w9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# resnet model\n",
        "# when tuning start with learning rate->mini_batch_size ->\n",
        "# momentum-> #hidden_units -> # learning_rate_decay -> #layers\n",
        "import tensorflow.keras as keras\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "import sklearn\n",
        "\n",
        "import matplotlib\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "pr7Djfco0136"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from builtins import print\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import operator\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from scipy.interpolate import interp1d\n",
        "from scipy.io import loadmat\n",
        "\n",
        "def calculate_metrics(y_true, y_pred, duration, y_true_val=None, y_pred_val=None):\n",
        "    res = pd.DataFrame(data=np.zeros((1, 4), dtype=float), index=[0],\n",
        "                       columns=['precision', 'accuracy', 'recall', 'duration'])\n",
        "    res['precision'] = precision_score(y_true, y_pred, average='macro')\n",
        "    res['accuracy'] = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    if not y_true_val is None:\n",
        "        # this is useful when transfer learning is used with cross validation\n",
        "        res['accuracy_val'] = accuracy_score(y_true_val, y_pred_val)\n",
        "\n",
        "    res['recall'] = recall_score(y_true, y_pred, average='macro')\n",
        "    res['duration'] = duration\n",
        "    return res\n",
        "\n",
        "\n",
        "def save_test_duration(file_name, test_duration):\n",
        "    res = pd.DataFrame(data=np.zeros((1, 1), dtype=float), index=[0],\n",
        "                       columns=['test_duration'])\n",
        "    res['test_duration'] = test_duration\n",
        "    res.to_csv(file_name, index=False)\n",
        "\n",
        "\n",
        "def plot_epochs_metric(hist, file_name, metric='loss'):\n",
        "    plt.figure()\n",
        "    plt.plot(hist.history[metric])\n",
        "    plt.plot(hist.history['val_' + metric])\n",
        "    plt.title('model ' + metric)\n",
        "    plt.ylabel(metric, fontsize='large')\n",
        "    plt.xlabel('epoch', fontsize='large')\n",
        "    plt.legend(['train', 'val'], loc='upper left')\n",
        "    plt.savefig(file_name, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def save_logs_t_leNet(output_directory, hist, y_pred, y_true, duration):\n",
        "    hist_df = pd.DataFrame(hist.history)\n",
        "    hist_df.to_csv(output_directory + 'history.csv', index=False)\n",
        "\n",
        "    df_metrics = calculate_metrics(y_true, y_pred, duration)\n",
        "    df_metrics.to_csv(output_directory + 'df_metrics.csv', index=False)\n",
        "\n",
        "    index_best_model = hist_df['loss'].idxmin()\n",
        "    row_best_model = hist_df.loc[index_best_model]\n",
        "\n",
        "    df_best_model = pd.DataFrame(data=np.zeros((1, 6), dtype=float), index=[0],\n",
        "                                 columns=['best_model_train_loss', 'best_model_val_loss', 'best_model_train_acc',\n",
        "                                          'best_model_val_acc', 'best_model_learning_rate', 'best_model_nb_epoch'])\n",
        "\n",
        "    df_best_model['best_model_train_loss'] = row_best_model['loss']\n",
        "    df_best_model['best_model_val_loss'] = row_best_model['val_loss']\n",
        "    df_best_model['best_model_train_acc'] = row_best_model['acc']\n",
        "    df_best_model['best_model_val_acc'] = row_best_model['val_acc']\n",
        "    df_best_model['best_model_nb_epoch'] = index_best_model\n",
        "\n",
        "    df_best_model.to_csv(output_directory + 'df_best_model.csv', index=False)\n",
        "\n",
        "    # plot losses\n",
        "    plot_epochs_metric(hist, output_directory + 'epochs_loss.png')\n",
        "\n",
        "\n",
        "def save_logs(output_directory, hist, y_pred, y_true, duration, lr=True, y_true_val=None, y_pred_val=None):\n",
        "    hist_df = pd.DataFrame(hist.history)\n",
        "    hist_df.to_csv(output_directory + 'history.csv', index=False)\n",
        "\n",
        "    df_metrics = calculate_metrics(y_true, y_pred, duration, y_true_val, y_pred_val)\n",
        "    df_metrics.to_csv(output_directory + 'df_metrics.csv', index=False)\n",
        "\n",
        "    index_best_model = hist_df['loss'].idxmin()\n",
        "    row_best_model = hist_df.loc[index_best_model]\n",
        "\n",
        "    df_best_model = pd.DataFrame(data=np.zeros((1, 6), dtype=float), index=[0],\n",
        "                                 columns=['best_model_train_loss', 'best_model_val_loss', 'best_model_train_acc',\n",
        "                                          'best_model_val_acc', 'best_model_learning_rate', 'best_model_nb_epoch'])\n",
        "\n",
        "    df_best_model['best_model_train_loss'] = row_best_model['loss']\n",
        "    df_best_model['best_model_val_loss'] = row_best_model['val_loss']\n",
        "    df_best_model['best_model_train_acc'] = row_best_model['accuracy']\n",
        "    df_best_model['best_model_val_acc'] = row_best_model['val_accuracy']\n",
        "    if lr == True:\n",
        "        df_best_model['best_model_learning_rate'] = row_best_model['lr']\n",
        "    df_best_model['best_model_nb_epoch'] = index_best_model\n",
        "\n",
        "    df_best_model.to_csv(output_directory + 'df_best_model.csv', index=False)\n",
        "\n",
        "    # for FCN there is no hyperparameters fine tuning - everything is static in code\n",
        "\n",
        "    # plot losses\n",
        "    plot_epochs_metric(hist, output_directory + 'epochs_loss.png')\n",
        "\n",
        "    return df_metrics"
      ],
      "metadata": {
        "id": "6kMBLhO6Ruuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Resnet classifier Regular"
      ],
      "metadata": {
        "id": "9zlrXeWP06XN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier_RESNET:\n",
        "\n",
        "    def __init__(self, output_directory, input_shape, nb_classes, verbose=False, build=True, load_weights=False):\n",
        "        self.output_directory = output_directory\n",
        "        if build == True:\n",
        "            self.model = self.build_model(input_shape, nb_classes)\n",
        "            if (verbose == True):\n",
        "                self.model.summary()\n",
        "            self.verbose = verbose\n",
        "            if load_weights == True:\n",
        "                self.model.load_weights(self.output_directory\n",
        "                                        .replace('resnet_augment', 'resnet')\n",
        "                                        .replace('TSC_itr_augment_x_10', 'TSC_itr_10')\n",
        "                                        + '/model_init.weights.h5')\n",
        "            else:\n",
        "                self.model.save_weights(self.output_directory + 'model_init.weights.h5')\n",
        "        return\n",
        "\n",
        "    def build_model(self, input_shape, nb_classes):\n",
        "        n_feature_maps = 64\n",
        "\n",
        "        input_layer = keras.layers.Input(input_shape)\n",
        "\n",
        "        # BLOCK 1\n",
        "\n",
        "        conv_x = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=8, padding='same')(input_layer)\n",
        "        conv_x = keras.layers.BatchNormalization()(conv_x)\n",
        "        conv_x = keras.layers.Activation('relu')(conv_x)\n",
        "\n",
        "        conv_y = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=5, padding='same')(conv_x)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "        conv_z = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=3, padding='same')(conv_y)\n",
        "        conv_z = keras.layers.BatchNormalization()(conv_z)\n",
        "\n",
        "        # expand channels for the sum\n",
        "        shortcut_y = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=1, padding='same')(input_layer)\n",
        "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
        "\n",
        "        output_block_1 = keras.layers.add([shortcut_y, conv_z])\n",
        "        output_block_1 = keras.layers.Activation('relu')(output_block_1)\n",
        "\n",
        "        # BLOCK 2\n",
        "\n",
        "        conv_x = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=8, padding='same')(output_block_1)\n",
        "        conv_x = keras.layers.BatchNormalization()(conv_x)\n",
        "        conv_x = keras.layers.Activation('relu')(conv_x)\n",
        "\n",
        "        conv_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "        conv_z = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n",
        "        conv_z = keras.layers.BatchNormalization()(conv_z)\n",
        "\n",
        "        # expand channels for the sum\n",
        "        shortcut_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=1, padding='same')(output_block_1)\n",
        "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
        "\n",
        "        output_block_2 = keras.layers.add([shortcut_y, conv_z])\n",
        "        output_block_2 = keras.layers.Activation('relu')(output_block_2)\n",
        "\n",
        "        # BLOCK 3\n",
        "\n",
        "        conv_x = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=8, padding='same')(output_block_2)\n",
        "        conv_x = keras.layers.BatchNormalization()(conv_x)\n",
        "        conv_x = keras.layers.Activation('relu')(conv_x)\n",
        "\n",
        "        conv_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "        conv_z = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n",
        "        conv_z = keras.layers.BatchNormalization()(conv_z)\n",
        "\n",
        "        # no need to expand channels because they are equal\n",
        "        shortcut_y = keras.layers.BatchNormalization()(output_block_2)\n",
        "\n",
        "        output_block_3 = keras.layers.add([shortcut_y, conv_z])\n",
        "        output_block_3 = keras.layers.Activation('relu')(output_block_3)\n",
        "\n",
        "        # FINAL\n",
        "\n",
        "        gap_layer = keras.layers.GlobalAveragePooling1D()(output_block_3)\n",
        "\n",
        "        output_layer = keras.layers.Dense(nb_classes, activation='softmax')(gap_layer)\n",
        "\n",
        "        model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "        model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(),\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=50, min_lr=0.0001)\n",
        "\n",
        "#         file_path = self.output_directory + 'best_model.{epoch:02d}-{val_loss:.2f}.weights.h5'\n",
        "\n",
        "#         model_checkpoint = keras.callbacks.ModelCheckpoint(\n",
        "#             filepath=file_path,\n",
        "#             monitor='val_accuracy',\n",
        "#             mode='max',\n",
        "#             verbose=1,\n",
        "#             save_best_only=True)\n",
        "\n",
        "        self.callbacks = [reduce_lr]\n",
        "\n",
        "        return model\n",
        "\n",
        "    def fit(self, x_train, y_train, x_val, y_val, y_true, batch_size, nb_epochs, shuffle, callbacks, class_weights):\n",
        "        if not tf.test.is_gpu_available:\n",
        "            print('error')\n",
        "            exit()\n",
        "\n",
        "        mini_batch_size = int(min(x_train.shape[0] / 10, batch_size))\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        hist = self.model.fit(x_train, y_train, batch_size=mini_batch_size, epochs=nb_epochs,\n",
        "                              verbose=1, validation_data=(x_val, y_val), callbacks=callbacks, shuffle=shuffle, class_weight=class_weights)\n",
        "\n",
        "        # duration = time.time() - start_time\n",
        "\n",
        "        # self.model.save(self.output_directory + 'last_model.h5')\n",
        "\n",
        "        # y_pred = self.predict(x_val, y_true, x_train, y_train, y_val,\n",
        "        #                       return_df_metrics=False)\n",
        "\n",
        "        # # save predictions\n",
        "        # np.save(self.output_directory + 'y_pred.npy', y_pred)\n",
        "\n",
        "        # # convert the predicted from binary to integer\n",
        "        # y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "        # df_metrics = save_logs(self.output_directory, hist, y_pred, y_true, duration)\n",
        "\n",
        "        # keras.backend.clear_session()\n",
        "\n",
        "        # return df_metrics\n",
        "\n",
        "    def predict(self, x_test, y_true, x_train, y_train, y_test, return_df_metrics=True):\n",
        "        start_time = time.time()\n",
        "        model_path = self.output_directory + 'best_model.weights.h5'\n",
        "        model = keras.models.load_model(model_path)\n",
        "        y_pred = model.predict(x_test)\n",
        "        if return_df_metrics:\n",
        "            y_pred = np.argmax(y_pred, axis=1)\n",
        "            df_metrics = calculate_metrics(y_true, y_pred, 0.0)\n",
        "            return df_metrics\n",
        "        else:\n",
        "            test_duration = time.time() - start_time\n",
        "            save_test_duration(self.output_directory + 'test_duration.csv', test_duration)\n",
        "            return y_pred"
      ],
      "metadata": {
        "id": "9cFsKzI_PhXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN-LSTM Original"
      ],
      "metadata": {
        "id": "g6bk8TWsyjby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Long short-term memory CNN-LSTM ###\n",
        "def lstm(n_timesteps, n_features, n_steps,  n_outputs):\n",
        "    model = Sequential()\n",
        "    model.add(TimeDistributed(Conv1D(filters=512, kernel_size=3, activation='relu'), input_shape=(n_steps, n_timesteps // n_steps, n_features)))\n",
        "\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(TimeDistributed(AveragePooling1D(pool_size=2)))\n",
        "    model.add(TimeDistributed(Conv1D(filters=128, kernel_size=3, activation='relu')))\n",
        "\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(TimeDistributed(Dropout(0.5)))\n",
        "    model.add(TimeDistributed(AveragePooling1D(pool_size=2)))\n",
        "    model.add(TimeDistributed(Conv1D(filters=32, kernel_size=3, activation='relu')))\n",
        "\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(TimeDistributed(Dropout(0.5)))\n",
        "    model.add(TimeDistributed(AveragePooling1D(pool_size=2)))\n",
        "    model.add(TimeDistributed(Conv1D(filters=8, kernel_size=3, activation='relu')))\n",
        "\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(TimeDistributed(Dropout(0.5)))\n",
        "    model.add(TimeDistributed(AveragePooling1D(pool_size=2)))\n",
        "    model.add(TimeDistributed(Flatten()))\n",
        "\n",
        "    model.add(layers.LSTM(16, return_sequences=True))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(layers.LSTM(16))\n",
        "    model.add(Dense(n_outputs, activation='softmax'))\n",
        "    return model"
      ],
      "metadata": {
        "id": "M1UajfXCkzs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regular LSTM"
      ],
      "metadata": {
        "id": "resZbFb6y17H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the Regular study LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=128, input_shape=(window_size, data.shape[1]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(units=8))\n",
        "model.add(Dense(units=len(unique_labels), activation='softmax'))  # Adjust the number of units based on your task"
      ],
      "metadata": {
        "id": "mQxLDLr7vTXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FCN (Fully convolutional network)"
      ],
      "metadata": {
        "id": "ehIaBuOfjBrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fcn(input_shape, num_classes):\n",
        "    input_layer = keras.layers.Input(input_shape)\n",
        "\n",
        "    conv1 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(input_layer)\n",
        "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
        "    conv1 = keras.layers.ReLU()(conv1)\n",
        "\n",
        "    conv2 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(conv1)\n",
        "    conv2 = keras.layers.BatchNormalization()(conv2)\n",
        "    conv2 = keras.layers.ReLU()(conv2)\n",
        "\n",
        "    conv3 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(conv2)\n",
        "    conv3 = keras.layers.BatchNormalization()(conv3)\n",
        "    conv3 = keras.layers.ReLU()(conv3)\n",
        "\n",
        "    gap = keras.layers.GlobalAveragePooling1D()(conv3)\n",
        "\n",
        "    output_layer = keras.layers.Dense(num_classes, activation=\"softmax\")(gap)\n",
        "\n",
        "    return keras.models.Model(inputs=input_layer, outputs=output_layer)"
      ],
      "metadata": {
        "id": "4admzamAi_zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inception"
      ],
      "metadata": {
        "id": "J4EcTkTtkzKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier_INCEPTION:\n",
        "\n",
        "    def __init__(self, output_directory, input_shape, nb_classes, verbose=False, build=True, batch_size=64, lr=0.001,\n",
        "                 nb_filters=32, use_residual=True, use_bottleneck=True, depth=6, kernel_size=41, nb_epochs=1500):\n",
        "\n",
        "        self.output_directory = output_directory\n",
        "\n",
        "        self.nb_filters = nb_filters\n",
        "        self.use_residual = use_residual\n",
        "        self.use_bottleneck = use_bottleneck\n",
        "        self.depth = depth\n",
        "        self.kernel_size = kernel_size - 1\n",
        "        self.callbacks = None\n",
        "        self.batch_size = batch_size\n",
        "        self.bottleneck_size = 32\n",
        "        self.nb_epochs = nb_epochs\n",
        "        self.lr = lr\n",
        "        self.verbose = verbose\n",
        "\n",
        "        if build == True:\n",
        "            self.model = self.build_model(input_shape, nb_classes)\n",
        "            if (verbose == True):\n",
        "                self.model.summary()\n",
        "            self.model.save_weights(self.output_directory + 'model_init.hdf5')\n",
        "\n",
        "    def _inception_module(self, input_tensor, stride=1, activation='linear'):\n",
        "\n",
        "        if self.use_bottleneck and int(input_tensor.shape[-1]) > self.bottleneck_size:\n",
        "            input_inception = keras.layers.Conv1D(filters=self.bottleneck_size, kernel_size=1,\n",
        "                                                  padding='same', activation=activation, use_bias=False)(input_tensor)\n",
        "        else:\n",
        "            input_inception = input_tensor\n",
        "\n",
        "        # kernel_size_s = [3, 5, 8, 11, 17]\n",
        "        kernel_size_s = [self.kernel_size // (2 ** i) for i in range(3)]\n",
        "\n",
        "        conv_list = []\n",
        "\n",
        "        for i in range(len(kernel_size_s)):\n",
        "            conv_list.append(keras.layers.Conv1D(filters=self.nb_filters, kernel_size=kernel_size_s[i],\n",
        "                                                 strides=stride, padding='same', activation=activation, use_bias=False)(\n",
        "                input_inception))\n",
        "\n",
        "        max_pool_1 = keras.layers.MaxPool1D(pool_size=3, strides=stride, padding='same')(input_tensor)\n",
        "\n",
        "        conv_6 = keras.layers.Conv1D(filters=self.nb_filters, kernel_size=1,\n",
        "                                     padding='same', activation=activation, use_bias=False)(max_pool_1)\n",
        "\n",
        "        conv_list.append(conv_6)\n",
        "\n",
        "        x = keras.layers.Concatenate(axis=2)(conv_list)\n",
        "        x = keras.layers.BatchNormalization()(x)\n",
        "        x = keras.layers.Activation(activation='relu')(x)\n",
        "        return x\n",
        "\n",
        "    def _shortcut_layer(self, input_tensor, out_tensor):\n",
        "        shortcut_y = keras.layers.Conv1D(filters=int(out_tensor.shape[-1]), kernel_size=1,\n",
        "                                         padding='same', use_bias=False)(input_tensor)\n",
        "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
        "\n",
        "        x = keras.layers.Add()([shortcut_y, out_tensor])\n",
        "        x = keras.layers.Activation('relu')(x)\n",
        "        return x\n",
        "\n",
        "    def build_model(self, input_shape, nb_classes):\n",
        "        input_layer = keras.layers.Input(input_shape)\n",
        "\n",
        "        x = input_layer\n",
        "        input_res = input_layer\n",
        "\n",
        "        for d in range(self.depth):\n",
        "\n",
        "            x = self._inception_module(x)\n",
        "\n",
        "            if self.use_residual and d % 3 == 2:\n",
        "                x = self._shortcut_layer(input_res, x)\n",
        "                input_res = x\n",
        "\n",
        "        gap_layer = keras.layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "        output_layer = keras.layers.Dense(nb_classes, activation='softmax')(gap_layer)\n",
        "\n",
        "        model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "        model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(self.lr),\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        # reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=20,\n",
        "        #                                               min_lr=0.0001)\n",
        "\n",
        "        # file_path = self.output_directory + 'best_model.{epoch:02d}-{val_loss:.2f}.h5'\n",
        "\n",
        "        # model_checkpoint = keras.callbacks.ModelCheckpoint(filepath=file_path, monitor='val_accuracy',\n",
        "        #                                                    save_best_only=True, verbose=1)\n",
        "\n",
        "        # self.callbacks = [reduce_lr, model_checkpoint]\n",
        "\n",
        "        return model\n",
        "\n",
        "    def fit(self, x_train, y_train, x_val, y_val, y_true, batch_size, nb_epochs, shuffle, callbacks, class_weights):\n",
        "        if not tf.test.is_gpu_available:\n",
        "            print('error no gpu')\n",
        "            exit()\n",
        "        # x_val and y_val are only used to monitor the test loss and NOT for training\n",
        "\n",
        "        if self.batch_size is None:\n",
        "            mini_batch_size = int(min(x_train.shape[0] / 10, 16))\n",
        "        else:\n",
        "            mini_batch_size = self.batch_size\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        hist = self.model.fit(x_train, y_train, batch_size=mini_batch_size, epochs=nb_epochs,\n",
        "                              verbose=self.verbose, validation_data=(x_val, y_val), callbacks=callbacks, shuffle=shuffle, class_weight=class_weights)\n",
        "\n",
        "        # duration = time.time() - start_time\n",
        "\n",
        "        # self.model.save(self.output_directory + 'last_model.hdf5')\n",
        "\n",
        "        # y_pred = self.predict(x_val, y_true, x_train, y_train, y_val,\n",
        "        #                       return_df_metrics=False)\n",
        "\n",
        "        # # save predictions\n",
        "        # np.save(self.output_directory + 'y_pred.npy', y_pred)\n",
        "\n",
        "        # # convert the predicted from binary to integer\n",
        "        # y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "        # df_metrics = save_logs(self.output_directory, hist, y_pred, y_true, duration)\n",
        "\n",
        "        # keras.backend.clear_session()\n",
        "\n",
        "        # return df_metrics\n",
        "\n",
        "    def predict(self, x_test, y_true, x_train, y_train, y_test, return_df_metrics=True):\n",
        "        start_time = time.time()\n",
        "        model_path = self.output_directory + 'best_model.hdf5'\n",
        "        model = keras.models.load_model(model_path)\n",
        "        y_pred = model.predict(x_test, batch_size=self.batch_size)\n",
        "        if return_df_metrics:\n",
        "            y_pred = np.argmax(y_pred, axis=1)\n",
        "            df_metrics = calculate_metrics(y_true, y_pred, 0.0)\n",
        "            return df_metrics\n",
        "        else:\n",
        "            test_duration = time.time() - start_time\n",
        "            save_test_duration(self.output_directory + 'test_duration.csv', test_duration)\n",
        "            return y_pred"
      ],
      "metadata": {
        "id": "ARJ79pgEk0Hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MLSTM-FCN"
      ],
      "metadata": {
        "id": "mFQ6uJQFtvrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, LSTM, multiply, concatenate, Activation, Masking, Reshape\n",
        "from keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout\n",
        "from keras import backend as K\n",
        "\n",
        "def mlstm_fcn(input_shape, num_classes):\n",
        "    ip = Input(input_shape)\n",
        "\n",
        "    x = Masking()(ip)\n",
        "    x = LSTM(8)(x)\n",
        "    x = Dropout(0.8)(x)\n",
        "\n",
        "    y = Permute((2, 1))(ip)\n",
        "    y = Conv1D(128, 8, padding='same', kernel_initializer='he_uniform')(y)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = Activation('relu')(y)\n",
        "    y = squeeze_excite_block(y)\n",
        "    y = Conv1D(256, 5, padding='same', kernel_initializer='he_uniform')(y)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = Activation('relu')(y)\n",
        "    y = squeeze_excite_block(y)\n",
        "\n",
        "    y = Conv1D(128, 3, padding='same', kernel_initializer='he_uniform')(y)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = Activation('relu')(y)\n",
        "\n",
        "    y = GlobalAveragePooling1D()(y)\n",
        "\n",
        "    x = concatenate([x, y])\n",
        "\n",
        "    out = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(ip, out)\n",
        "    # add load model code here to fine-tune\n",
        "\n",
        "    return model\n",
        "\n",
        "def squeeze_excite_block(input):\n",
        "    ''' Create a squeeze-excite block\n",
        "    Args:\n",
        "        input: input tensor\n",
        "        filters: number of output filters\n",
        "        k: width factor\n",
        "\n",
        "    Returns: a keras tensor\n",
        "    '''\n",
        "    # filters = tf.shape(input)[-1] # channel_axis = -1 for TF\n",
        "    filters = K.int_shape(input)[-1]\n",
        "\n",
        "    se = GlobalAveragePooling1D()(input)\n",
        "    # se = Reshape((1, filters))(se)\n",
        "    se = K.reshape(se, (-1, 1, filters))\n",
        "    se = Dense(filters // 16,  activation='relu', kernel_initializer='he_normal', use_bias=False)(se)\n",
        "    se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\n",
        "    se = multiply([input, se])\n",
        "    return se"
      ],
      "metadata": {
        "id": "-LxqPYnUtxbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MultiRocket"
      ],
      "metadata": {
        "id": "SiROGmfIzn3Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Helpers"
      ],
      "metadata": {
        "id": "TYa7km-jpF7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "class LogisticRegression:\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            num_features,\n",
        "            max_epochs=500,\n",
        "            minibatch_size=256,\n",
        "            validation_size=2 ** 11,\n",
        "            learning_rate=1e-4,\n",
        "            patience_lr=5,  # 500 minibatches\n",
        "            patience=10,  # 1000 minibatches\n",
        "            device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    ):\n",
        "        self.name = \"LogisticRegression\"\n",
        "        self.args = {\n",
        "            \"num_features\": num_features,\n",
        "            \"validation_size\": validation_size,\n",
        "            \"minibatch_size\": minibatch_size,\n",
        "            \"lr\": learning_rate,\n",
        "            \"max_epochs\": max_epochs,\n",
        "            \"patience_lr\": patience_lr,\n",
        "            \"patience\": patience,\n",
        "        }\n",
        "\n",
        "        self.model = None\n",
        "        self.device = device\n",
        "        self.classes = None\n",
        "        self.scaler = None\n",
        "        self.num_classes = None\n",
        "\n",
        "    def fit(self, x_train, y_train):\n",
        "        self.classes = np.unique(y_train)\n",
        "        self.num_classes = len(self.classes)\n",
        "\n",
        "        num_outputs = self.num_classes if self.num_classes > 2 else 1\n",
        "        train_steps = int(x_train.shape[0] / self.args[\"minibatch_size\"])\n",
        "\n",
        "        self.scaler = StandardScaler()\n",
        "        x_train = self.scaler.fit_transform(x_train)\n",
        "\n",
        "        model = torch.nn.Sequential(torch.nn.Linear(self.args[\"num_features\"], num_outputs)).to(self.device)\n",
        "\n",
        "        if num_outputs == 1:\n",
        "            loss_function = torch.nn.BCEWithLogitsLoss()\n",
        "        else:\n",
        "            loss_function = torch.nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=self.args[\"lr\"])\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer,\n",
        "            factor=0.5,\n",
        "            min_lr=1e-8,\n",
        "            patience=self.args[\"patience_lr\"]\n",
        "        )\n",
        "\n",
        "        training_size = x_train.shape[0]\n",
        "        if self.args[\"validation_size\"] < training_size:\n",
        "            x_training, x_validation, y_training, y_validation = train_test_split(\n",
        "                x_train, y_train,\n",
        "                test_size=self.args[\"validation_size\"],\n",
        "                stratify=y_train\n",
        "            )\n",
        "\n",
        "            train_data = TensorDataset(\n",
        "                torch.tensor(x_training, dtype=torch.float32).to(self.device),\n",
        "                torch.tensor(y_training, dtype=torch.long).to(self.device)\n",
        "            )\n",
        "            val_data = TensorDataset(\n",
        "                torch.tensor(x_validation, dtype=torch.float32).to(self.device),\n",
        "                torch.tensor(y_validation, dtype=torch.long).to(self.device)\n",
        "            )\n",
        "            train_dataloader = DataLoader(train_data, shuffle=True, batch_size=self.args[\"minibatch_size\"])\n",
        "            val_dataloader = DataLoader(val_data, batch_size=self.args[\"minibatch_size\"])\n",
        "        else:\n",
        "            train_data = TensorDataset(\n",
        "                torch.tensor(x_train, dtype=torch.float32).to(self.device),\n",
        "                torch.tensor(y_train, dtype=torch.long).to(self.device)\n",
        "            )\n",
        "            train_dataloader = DataLoader(train_data, shuffle=True, batch_size=self.args[\"minibatch_size\"])\n",
        "            val_dataloader = None\n",
        "\n",
        "        best_loss = np.inf\n",
        "        best_model = None\n",
        "        stall_count = 0\n",
        "        stop = False\n",
        "\n",
        "        for epoch in range(self.args[\"max_epochs\"]):\n",
        "            if epoch > 0 and stop:\n",
        "                break\n",
        "            model.train()\n",
        "\n",
        "            # loop over the training set\n",
        "            total_train_loss = 0\n",
        "            steps = 0\n",
        "            for i, data in tqdm(enumerate(train_dataloader), desc=f\"epoch: {epoch}\", total=train_steps):\n",
        "                x, y = data\n",
        "\n",
        "                y_hat = model(x)\n",
        "                if num_outputs == 1:\n",
        "                    loss = loss_function(y_hat.sigmoid(), y)\n",
        "                else:\n",
        "                    yhat = torch.nn.functional.softmax(y_hat, dim=1)\n",
        "                    loss = loss_function(yhat, y)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_train_loss += loss\n",
        "                steps += 1\n",
        "\n",
        "            total_train_loss = total_train_loss.cpu().detach().numpy() / steps\n",
        "\n",
        "            if val_dataloader is not None:\n",
        "                total_val_loss = 0\n",
        "                # switch off autograd for evaluation\n",
        "                with torch.no_grad():\n",
        "                    # set the model in evaluation mode\n",
        "                    model.eval()\n",
        "                    for i, data in enumerate(val_dataloader):\n",
        "                        x, y = data\n",
        "\n",
        "                        y_hat = model(x)\n",
        "                        if num_outputs == 1:\n",
        "                            total_val_loss += loss_function(y_hat.sigmoid(), y)\n",
        "                        else:\n",
        "                            yhat = torch.nn.functional.softmax(y_hat, dim=1)\n",
        "                            total_val_loss += loss_function(yhat, y)\n",
        "                total_val_loss = total_val_loss.cpu().detach().numpy() / steps\n",
        "                scheduler.step(total_val_loss)\n",
        "\n",
        "                if total_val_loss >= best_loss:\n",
        "                    stall_count += 1\n",
        "                    if stall_count >= self.args[\"patience\"]:\n",
        "                        stop = True\n",
        "                        print(f\"\\n<Stopped at Epoch {epoch + 1}>\")\n",
        "                else:\n",
        "                    best_loss = total_val_loss\n",
        "                    best_model = copy.deepcopy(model)\n",
        "                    if not stop:\n",
        "                        stall_count = 0\n",
        "            else:\n",
        "                scheduler.step(total_train_loss)\n",
        "                if total_train_loss >= best_loss:\n",
        "                    stall_count += 1\n",
        "                    if stall_count >= self.args[\"patience\"]:\n",
        "                        stop = True\n",
        "                        print(f\"\\n<Stopped at Epoch {epoch + 1}>\")\n",
        "                else:\n",
        "                    best_loss = total_train_loss\n",
        "                    best_model = copy.deepcopy(model)\n",
        "                    if not stop:\n",
        "                        stall_count = 0\n",
        "\n",
        "        self.model = best_model\n",
        "        return self.model\n",
        "\n",
        "    def predict(self, x):\n",
        "        x = self.scaler.transform(x)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # set the model in evaluation mode\n",
        "            self.model.eval()\n",
        "\n",
        "            yhat = self.model(torch.tensor(x, dtype=torch.float32).to(self.device))\n",
        "\n",
        "            if self.num_classes > 2:\n",
        "                yhat = self.classes[np.argmax(yhat.cpu().detach().numpy(), axis=1)]\n",
        "            else:\n",
        "                yhat = torch.sigmoid(yhat)\n",
        "                yhat = np.round(yhat.cpu().detach().numpy())\n",
        "\n",
        "            return yhat"
      ],
      "metadata": {
        "id": "Th8aT1m9z1Ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fill_missing(x: np.array,\n",
        "                 max_len: int,\n",
        "                 vary_len: str = \"suffix-noise\",\n",
        "                 normalise: bool = True):\n",
        "    if vary_len == \"zero\":\n",
        "        if normalise:\n",
        "            x = StandardScaler().fit_transform(x)\n",
        "        x = np.nan_to_num(x)\n",
        "    elif vary_len == 'prefix-suffix-noise':\n",
        "        for i in range(len(x)):\n",
        "            series = list()\n",
        "            for a in x[i, :]:\n",
        "                if np.isnan(a):\n",
        "                    break\n",
        "                series.append(a)\n",
        "            series = np.array(series)\n",
        "            seq_len = len(series)\n",
        "            diff_len = int(0.5 * (max_len - seq_len))\n",
        "\n",
        "            for j in range(diff_len):\n",
        "                x[i, j] = random.random() / 1000\n",
        "\n",
        "            for j in range(diff_len, seq_len):\n",
        "                x[i, j] = series[j - seq_len]\n",
        "\n",
        "            for j in range(seq_len, max_len):\n",
        "                x[i, j] = random.random() / 1000\n",
        "\n",
        "            if normalise:\n",
        "                tmp = StandardScaler().fit_transform(x[i].reshape(-1, 1))\n",
        "                x[i] = tmp[:, 0]\n",
        "    elif vary_len == 'uniform-scaling':\n",
        "        for i in range(len(x)):\n",
        "            series = list()\n",
        "            for a in x[i, :]:\n",
        "                if np.isnan(a):\n",
        "                    break\n",
        "                series.append(a)\n",
        "            series = np.array(series)\n",
        "            seq_len = len(series)\n",
        "\n",
        "            for j in range(max_len):\n",
        "                scaling_factor = int(j * seq_len / max_len)\n",
        "                x[i, j] = series[scaling_factor]\n",
        "            if normalise:\n",
        "                tmp = StandardScaler().fit_transform(x[i].reshape(-1, 1))\n",
        "                x[i] = tmp[:, 0]\n",
        "    else:\n",
        "        for i in range(len(x)):\n",
        "            for j in range(len(x[i])):\n",
        "                if np.isnan(x[i, j]):\n",
        "                    x[i, j] = random.random() / 1000\n",
        "\n",
        "            if normalise:\n",
        "                tmp = StandardScaler().fit_transform(x[i].reshape(-1, 1))\n",
        "                x[i] = tmp[:, 0]\n",
        "\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "Z5XD3cgZpPKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_ts_data(X,\n",
        "                    vary_len: str = \"suffix-noise\",\n",
        "                    normalise: bool = False):\n",
        "    \"\"\"\n",
        "    This is a function to process the data, i.e. convert dataframe to numpy array\n",
        "    :param X:\n",
        "    :param normalise:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    num_instances, num_dim = X.shape\n",
        "    columns = X.columns\n",
        "    max_len = np.max([len(X[columns[0]][i]) for i in range(num_instances)])\n",
        "    output = np.zeros((num_instances, num_dim, max_len), dtype=np.float64)\n",
        "\n",
        "    for i in range(num_dim):\n",
        "        for j in range(num_instances):\n",
        "            output[j, i, :] = X[columns[i]][j].values\n",
        "        output[:, i, :] = fill_missing(\n",
        "            output[:, i, :],\n",
        "            max_len,\n",
        "            vary_len,\n",
        "            normalise\n",
        "        )\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "kSBROJzjpP5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Architecture"
      ],
      "metadata": {
        "id": "EpEYtIiopIv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chang Wei Tan, Angus Dempster, Christoph Bergmeir, Geoffrey I Webb\n",
        "#\n",
        "# MultiRocket: Multiple pooling operators and transformations for fast and effective time series classification\n",
        "# https://arxiv.org/abs/2102.00457\n",
        "\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "from numba import njit, prange\n",
        "from sklearn.linear_model import RidgeClassifierCV\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "@njit(\"float32[:](float64[:,:,:],int32[:],int32[:],int32[:],int32[:],float32[:])\",\n",
        "      fastmath=True, parallel=False, cache=True)\n",
        "def _fit_biases(X, num_channels_per_combination, channel_indices, dilations, num_features_per_dilation, quantiles):\n",
        "    num_examples, num_channels, input_length = X.shape\n",
        "\n",
        "    # equivalent to:\n",
        "    # >>> from itertools import combinations\n",
        "    # >>> indices = np.array([_ for _ in combinations(np.arange(9), 3)], dtype = np.int32)\n",
        "    indices = np.array((\n",
        "        0, 1, 2, 0, 1, 3, 0, 1, 4, 0, 1, 5, 0, 1, 6, 0, 1, 7, 0, 1, 8,\n",
        "        0, 2, 3, 0, 2, 4, 0, 2, 5, 0, 2, 6, 0, 2, 7, 0, 2, 8, 0, 3, 4,\n",
        "        0, 3, 5, 0, 3, 6, 0, 3, 7, 0, 3, 8, 0, 4, 5, 0, 4, 6, 0, 4, 7,\n",
        "        0, 4, 8, 0, 5, 6, 0, 5, 7, 0, 5, 8, 0, 6, 7, 0, 6, 8, 0, 7, 8,\n",
        "        1, 2, 3, 1, 2, 4, 1, 2, 5, 1, 2, 6, 1, 2, 7, 1, 2, 8, 1, 3, 4,\n",
        "        1, 3, 5, 1, 3, 6, 1, 3, 7, 1, 3, 8, 1, 4, 5, 1, 4, 6, 1, 4, 7,\n",
        "        1, 4, 8, 1, 5, 6, 1, 5, 7, 1, 5, 8, 1, 6, 7, 1, 6, 8, 1, 7, 8,\n",
        "        2, 3, 4, 2, 3, 5, 2, 3, 6, 2, 3, 7, 2, 3, 8, 2, 4, 5, 2, 4, 6,\n",
        "        2, 4, 7, 2, 4, 8, 2, 5, 6, 2, 5, 7, 2, 5, 8, 2, 6, 7, 2, 6, 8,\n",
        "        2, 7, 8, 3, 4, 5, 3, 4, 6, 3, 4, 7, 3, 4, 8, 3, 5, 6, 3, 5, 7,\n",
        "        3, 5, 8, 3, 6, 7, 3, 6, 8, 3, 7, 8, 4, 5, 6, 4, 5, 7, 4, 5, 8,\n",
        "        4, 6, 7, 4, 6, 8, 4, 7, 8, 5, 6, 7, 5, 6, 8, 5, 7, 8, 6, 7, 8\n",
        "    ), dtype=np.int32).reshape(84, 3)\n",
        "\n",
        "    num_kernels = len(indices)\n",
        "    num_dilations = len(dilations)\n",
        "\n",
        "    num_features = num_kernels * np.sum(num_features_per_dilation)\n",
        "\n",
        "    biases = np.zeros(num_features, dtype=np.float32)\n",
        "\n",
        "    feature_index_start = 0\n",
        "\n",
        "    combination_index = 0\n",
        "    num_channels_start = 0\n",
        "\n",
        "    for dilation_index in range(num_dilations):\n",
        "\n",
        "        dilation = dilations[dilation_index]\n",
        "        padding = ((9 - 1) * dilation) // 2\n",
        "\n",
        "        num_features_this_dilation = num_features_per_dilation[dilation_index]\n",
        "\n",
        "        for kernel_index in range(num_kernels):\n",
        "\n",
        "            feature_index_end = feature_index_start + num_features_this_dilation\n",
        "\n",
        "            num_channels_this_combination = num_channels_per_combination[combination_index]\n",
        "\n",
        "            num_channels_end = num_channels_start + num_channels_this_combination\n",
        "\n",
        "            channels_this_combination = channel_indices[num_channels_start:num_channels_end]\n",
        "\n",
        "            _X = X[np.random.randint(num_examples)][channels_this_combination]\n",
        "\n",
        "            A = -_X  # A = alpha * X = -X\n",
        "            G = _X + _X + _X  # G = gamma * X = 3X\n",
        "\n",
        "            C_alpha = np.zeros((num_channels_this_combination, input_length), dtype=np.float32)\n",
        "            C_alpha[:] = A\n",
        "\n",
        "            C_gamma = np.zeros((9, num_channels_this_combination, input_length), dtype=np.float32)\n",
        "            C_gamma[9 // 2] = G\n",
        "\n",
        "            start = dilation\n",
        "            end = input_length - padding\n",
        "\n",
        "            for gamma_index in range(9 // 2):\n",
        "                C_alpha[:, -end:] = C_alpha[:, -end:] + A[:, :end]\n",
        "                C_gamma[gamma_index, :, -end:] = G[:, :end]\n",
        "\n",
        "                end += dilation\n",
        "\n",
        "            for gamma_index in range(9 // 2 + 1, 9):\n",
        "                C_alpha[:, :-start] = C_alpha[:, :-start] + A[:, start:]\n",
        "                C_gamma[gamma_index, :, :-start] = G[:, start:]\n",
        "\n",
        "                start += dilation\n",
        "\n",
        "            index_0, index_1, index_2 = indices[kernel_index]\n",
        "\n",
        "            C = C_alpha + C_gamma[index_0] + C_gamma[index_1] + C_gamma[index_2]\n",
        "            C = np.sum(C, axis=0)\n",
        "\n",
        "            biases[feature_index_start:feature_index_end] = np.quantile(C, quantiles[\n",
        "                                                                           feature_index_start:feature_index_end])\n",
        "\n",
        "            feature_index_start = feature_index_end\n",
        "\n",
        "            combination_index += 1\n",
        "            num_channels_start = num_channels_end\n",
        "\n",
        "    return biases\n",
        "\n",
        "\n",
        "def _fit_dilations(input_length, num_features, max_dilations_per_kernel):\n",
        "    num_kernels = 84\n",
        "\n",
        "    num_features_per_kernel = num_features // num_kernels\n",
        "    true_max_dilations_per_kernel = min(num_features_per_kernel, max_dilations_per_kernel)\n",
        "    multiplier = num_features_per_kernel / true_max_dilations_per_kernel\n",
        "\n",
        "    print(\"TEST3\")\n",
        "    max_exponent = np.log2((input_length - 1) / (9 - 1))\n",
        "    dilations, num_features_per_dilation = \\\n",
        "        np.unique(np.logspace(0, max_exponent, true_max_dilations_per_kernel, base=2).astype(np.int32),\n",
        "                  return_counts=True)\n",
        "    num_features_per_dilation = (num_features_per_dilation * multiplier).astype(np.int32)  # this is a vector\n",
        "\n",
        "    remainder = num_features_per_kernel - np.sum(num_features_per_dilation)\n",
        "    i = 0\n",
        "    while remainder > 0:\n",
        "        num_features_per_dilation[i] += 1\n",
        "        remainder -= 1\n",
        "        i = (i + 1) % len(num_features_per_dilation)\n",
        "\n",
        "    return dilations, num_features_per_dilation\n",
        "\n",
        "\n",
        "# low-discrepancy sequence to assign quantiles to kernel/dilation combinations\n",
        "def _quantiles(n):\n",
        "    return np.array([(_ * ((np.sqrt(5) + 1) / 2)) % 1 for _ in range(1, n + 1)], dtype=np.float32)\n",
        "\n",
        "\n",
        "def fit(X, num_features=10_000, max_dilations_per_kernel=32):\n",
        "    _, num_channels, input_length = X.shape\n",
        "\n",
        "    num_kernels = 84\n",
        "\n",
        "    dilations, num_features_per_dilation = _fit_dilations(input_length, num_features, max_dilations_per_kernel)\n",
        "\n",
        "    num_features_per_kernel = np.sum(num_features_per_dilation)\n",
        "\n",
        "    quantiles = _quantiles(num_kernels * num_features_per_kernel)\n",
        "\n",
        "    num_dilations = len(dilations)\n",
        "    num_combinations = num_kernels * num_dilations\n",
        "\n",
        "    max_num_channels = min(num_channels, 9)\n",
        "    max_exponent = np.log2(max_num_channels + 1)\n",
        "\n",
        "    num_channels_per_combination = (2 ** np.random.uniform(0, max_exponent, num_combinations)).astype(np.int32)\n",
        "\n",
        "    channel_indices = np.zeros(num_channels_per_combination.sum(), dtype=np.int32)\n",
        "\n",
        "    num_channels_start = 0\n",
        "    for combination_index in range(num_combinations):\n",
        "        num_channels_this_combination = num_channels_per_combination[combination_index]\n",
        "        num_channels_end = num_channels_start + num_channels_this_combination\n",
        "        channel_indices[num_channels_start:num_channels_end] = np.random.choice(num_channels,\n",
        "                                                                                num_channels_this_combination,\n",
        "                                                                                replace=False)\n",
        "\n",
        "        num_channels_start = num_channels_end\n",
        "\n",
        "    biases = _fit_biases(X, num_channels_per_combination, channel_indices,\n",
        "                         dilations, num_features_per_dilation, quantiles)\n",
        "\n",
        "    return num_channels_per_combination, channel_indices, dilations, num_features_per_dilation, biases\n",
        "\n",
        "\n",
        "@njit(\n",
        "    \"float32[:,:](float64[:,:,:],float64[:,:,:],Tuple((int32[:],int32[:],int32[:],int32[:],float32[:])),Tuple((int32[:],int32[:],int32[:],int32[:],float32[:])),int32)\",\n",
        "    fastmath=True, parallel=True, cache=True)\n",
        "def transform(X, X1, parameters, parameters1, n_features_per_kernel=4):\n",
        "    num_examples, num_channels, input_length = X.shape\n",
        "\n",
        "    num_channels_per_combination, channel_indices, dilations, num_features_per_dilation, biases = parameters\n",
        "    _, _, dilations1, num_features_per_dilation1, biases1 = parameters1\n",
        "\n",
        "    # equivalent to:\n",
        "    # >>> from itertools import combinations\n",
        "    # >>> indices = np.array([_ for _ in combinations(np.arange(9), 3)], dtype = np.int32)\n",
        "    indices = np.array((\n",
        "        0, 1, 2, 0, 1, 3, 0, 1, 4, 0, 1, 5, 0, 1, 6, 0, 1, 7, 0, 1, 8,\n",
        "        0, 2, 3, 0, 2, 4, 0, 2, 5, 0, 2, 6, 0, 2, 7, 0, 2, 8, 0, 3, 4,\n",
        "        0, 3, 5, 0, 3, 6, 0, 3, 7, 0, 3, 8, 0, 4, 5, 0, 4, 6, 0, 4, 7,\n",
        "        0, 4, 8, 0, 5, 6, 0, 5, 7, 0, 5, 8, 0, 6, 7, 0, 6, 8, 0, 7, 8,\n",
        "        1, 2, 3, 1, 2, 4, 1, 2, 5, 1, 2, 6, 1, 2, 7, 1, 2, 8, 1, 3, 4,\n",
        "        1, 3, 5, 1, 3, 6, 1, 3, 7, 1, 3, 8, 1, 4, 5, 1, 4, 6, 1, 4, 7,\n",
        "        1, 4, 8, 1, 5, 6, 1, 5, 7, 1, 5, 8, 1, 6, 7, 1, 6, 8, 1, 7, 8,\n",
        "        2, 3, 4, 2, 3, 5, 2, 3, 6, 2, 3, 7, 2, 3, 8, 2, 4, 5, 2, 4, 6,\n",
        "        2, 4, 7, 2, 4, 8, 2, 5, 6, 2, 5, 7, 2, 5, 8, 2, 6, 7, 2, 6, 8,\n",
        "        2, 7, 8, 3, 4, 5, 3, 4, 6, 3, 4, 7, 3, 4, 8, 3, 5, 6, 3, 5, 7,\n",
        "        3, 5, 8, 3, 6, 7, 3, 6, 8, 3, 7, 8, 4, 5, 6, 4, 5, 7, 4, 5, 8,\n",
        "        4, 6, 7, 4, 6, 8, 4, 7, 8, 5, 6, 7, 5, 6, 8, 5, 7, 8, 6, 7, 8\n",
        "    ), dtype=np.int32).reshape(84, 3)\n",
        "\n",
        "    num_kernels = len(indices)\n",
        "    num_dilations = len(dilations)\n",
        "    num_dilations1 = len(dilations1)\n",
        "\n",
        "    num_features = num_kernels * np.sum(num_features_per_dilation)\n",
        "    num_features1 = num_kernels * np.sum(num_features_per_dilation1)\n",
        "\n",
        "    features = np.zeros((num_examples, (num_features + num_features1) * n_features_per_kernel), dtype=np.float32)\n",
        "    n_features_per_transform = np.int64(features.shape[1] / 2)\n",
        "\n",
        "    for example_index in prange(num_examples):\n",
        "\n",
        "        _X = X[example_index]\n",
        "\n",
        "        A = -_X  # A = alpha * X = -X\n",
        "        G = _X + _X + _X  # G = gamma * X = 3X\n",
        "\n",
        "        # Base series\n",
        "        feature_index_start = 0\n",
        "\n",
        "        combination_index = 0\n",
        "        num_channels_start = 0\n",
        "\n",
        "        for dilation_index in range(num_dilations):\n",
        "\n",
        "            _padding0 = dilation_index % 2\n",
        "\n",
        "            dilation = dilations[dilation_index]\n",
        "            padding = ((9 - 1) * dilation) // 2\n",
        "\n",
        "            num_features_this_dilation = num_features_per_dilation[dilation_index]\n",
        "\n",
        "            C_alpha = np.zeros((num_channels, input_length), dtype=np.float32)\n",
        "            C_alpha[:] = A\n",
        "\n",
        "            C_gamma = np.zeros((9, num_channels, input_length), dtype=np.float32)\n",
        "            C_gamma[9 // 2] = G\n",
        "\n",
        "            start = dilation\n",
        "            end = input_length - padding\n",
        "\n",
        "            for gamma_index in range(9 // 2):\n",
        "                C_alpha[:, -end:] = C_alpha[:, -end:] + A[:, :end]\n",
        "                C_gamma[gamma_index, :, -end:] = G[:, :end]\n",
        "\n",
        "                end += dilation\n",
        "\n",
        "            for gamma_index in range(9 // 2 + 1, 9):\n",
        "                C_alpha[:, :-start] = C_alpha[:, :-start] + A[:, start:]\n",
        "                C_gamma[gamma_index, :, :-start] = G[:, start:]\n",
        "\n",
        "                start += dilation\n",
        "\n",
        "            for kernel_index in range(num_kernels):\n",
        "\n",
        "                feature_index_end = feature_index_start + num_features_this_dilation\n",
        "\n",
        "                num_channels_this_combination = num_channels_per_combination[combination_index]\n",
        "\n",
        "                num_channels_end = num_channels_start + num_channels_this_combination\n",
        "\n",
        "                channels_this_combination = channel_indices[num_channels_start:num_channels_end]\n",
        "\n",
        "                _padding1 = (_padding0 + kernel_index) % 2\n",
        "\n",
        "                index_0, index_1, index_2 = indices[kernel_index]\n",
        "\n",
        "                C = C_alpha[channels_this_combination] + \\\n",
        "                    C_gamma[index_0][channels_this_combination] + \\\n",
        "                    C_gamma[index_1][channels_this_combination] + \\\n",
        "                    C_gamma[index_2][channels_this_combination]\n",
        "                C = np.sum(C, axis=0)\n",
        "\n",
        "                if _padding1 == 0:\n",
        "                    for feature_count in range(num_features_this_dilation):\n",
        "                        feature_index = feature_index_start + feature_count\n",
        "                        _bias = biases[feature_index]\n",
        "\n",
        "                        ppv = 0\n",
        "                        last_val = 0\n",
        "                        max_stretch = 0.0\n",
        "                        mean_index = 0\n",
        "                        mean = 0\n",
        "\n",
        "                        for j in range(C.shape[0]):\n",
        "                            if C[j] > _bias:\n",
        "                                ppv += 1\n",
        "                                mean_index += j\n",
        "                                mean += C[j] + _bias\n",
        "                            elif C[j] < _bias:\n",
        "                                stretch = j - last_val\n",
        "                                if stretch > max_stretch:\n",
        "                                    max_stretch = stretch\n",
        "                                last_val = j\n",
        "                        stretch = C.shape[0] - 1 - last_val\n",
        "                        if stretch > max_stretch:\n",
        "                            max_stretch = stretch\n",
        "\n",
        "                        end = feature_index\n",
        "                        features[example_index, end] = ppv / C.shape[0]\n",
        "                        end = end + num_features\n",
        "                        features[example_index, end] = max_stretch\n",
        "                        end = end + num_features\n",
        "                        features[example_index, end] = mean / ppv if ppv > 0 else 0\n",
        "                        end = end + num_features\n",
        "                        features[example_index, end] = mean_index / ppv if ppv > 0 else -1\n",
        "                else:\n",
        "                    _c = C[padding:-padding]\n",
        "\n",
        "                    for feature_count in range(num_features_this_dilation):\n",
        "                        feature_index = feature_index_start + feature_count\n",
        "                        _bias = biases[feature_index]\n",
        "\n",
        "                        ppv = 0\n",
        "                        last_val = 0\n",
        "                        max_stretch = 0.0\n",
        "                        mean_index = 0\n",
        "                        mean = 0\n",
        "\n",
        "                        for j in range(_c.shape[0]):\n",
        "                            if _c[j] > _bias:\n",
        "                                ppv += 1\n",
        "                                mean_index += j\n",
        "                                mean += _c[j] + _bias\n",
        "                            elif _c[j] < _bias:\n",
        "                                stretch = j - last_val\n",
        "                                if stretch > max_stretch:\n",
        "                                    max_stretch = stretch\n",
        "                                last_val = j\n",
        "                        stretch = _c.shape[0] - 1 - last_val\n",
        "                        if stretch > max_stretch:\n",
        "                            max_stretch = stretch\n",
        "\n",
        "                        end = feature_index\n",
        "                        features[example_index, end] = ppv / _c.shape[0]\n",
        "                        end = end + num_features\n",
        "                        features[example_index, end] = max_stretch\n",
        "                        end = end + num_features\n",
        "                        features[example_index, end] = mean / ppv if ppv > 0 else 0\n",
        "                        end = end + num_features\n",
        "                        features[example_index, end] = mean_index / ppv if ppv > 0 else -1\n",
        "\n",
        "                feature_index_start = feature_index_end\n",
        "\n",
        "                combination_index += 1\n",
        "                num_channels_start = num_channels_end\n",
        "\n",
        "        # First order difference\n",
        "        _X1 = X1[example_index]\n",
        "        A1 = -_X1  # A = alpha * X = -X\n",
        "        G1 = _X1 + _X1 + _X1  # G = gamma * X = 3X\n",
        "\n",
        "        feature_index_start = 0\n",
        "\n",
        "        combination_index = 0\n",
        "        num_channels_start = 0\n",
        "\n",
        "        for dilation_index in range(num_dilations1):\n",
        "\n",
        "            _padding0 = dilation_index % 2\n",
        "\n",
        "            dilation = dilations1[dilation_index]\n",
        "            padding = ((9 - 1) * dilation) // 2\n",
        "\n",
        "            num_features_this_dilation = num_features_per_dilation1[dilation_index]\n",
        "\n",
        "            C_alpha = np.zeros((num_channels, input_length - 1), dtype=np.float32)\n",
        "            C_alpha[:] = A1\n",
        "\n",
        "            C_gamma = np.zeros((9, num_channels, input_length - 1), dtype=np.float32)\n",
        "            C_gamma[9 // 2] = G1\n",
        "\n",
        "            start = dilation\n",
        "            end = input_length - padding\n",
        "\n",
        "            for gamma_index in range(9 // 2):\n",
        "                C_alpha[:, -end:] = C_alpha[:, -end:] + A1[:, :end]\n",
        "                C_gamma[gamma_index, :, -end:] = G1[:, :end]\n",
        "\n",
        "                end += dilation\n",
        "\n",
        "            for gamma_index in range(9 // 2 + 1, 9):\n",
        "                C_alpha[:, :-start] = C_alpha[:, :-start] + A1[:, start:]\n",
        "                C_gamma[gamma_index, :, :-start] = G1[:, start:]\n",
        "\n",
        "                start += dilation\n",
        "\n",
        "            for kernel_index in range(num_kernels):\n",
        "\n",
        "                feature_index_end = feature_index_start + num_features_this_dilation\n",
        "\n",
        "                num_channels_this_combination = num_channels_per_combination[combination_index]\n",
        "\n",
        "                num_channels_end = num_channels_start + num_channels_this_combination\n",
        "\n",
        "                channels_this_combination = channel_indices[num_channels_start:num_channels_end]\n",
        "\n",
        "                _padding1 = (_padding0 + kernel_index) % 2\n",
        "\n",
        "                index_0, index_1, index_2 = indices[kernel_index]\n",
        "\n",
        "                C = C_alpha[channels_this_combination] + \\\n",
        "                    C_gamma[index_0][channels_this_combination] + \\\n",
        "                    C_gamma[index_1][channels_this_combination] + \\\n",
        "                    C_gamma[index_2][channels_this_combination]\n",
        "                C = np.sum(C, axis=0)\n",
        "\n",
        "                if _padding1 == 0:\n",
        "                    for feature_count in range(num_features_this_dilation):\n",
        "                        feature_index = feature_index_start + feature_count\n",
        "                        _bias = biases1[feature_index]\n",
        "\n",
        "                        ppv = 0\n",
        "                        last_val = 0\n",
        "                        max_stretch = 0.0\n",
        "                        mean_index = 0\n",
        "                        mean = 0\n",
        "\n",
        "                        for j in range(C.shape[0]):\n",
        "                            if C[j] > _bias:\n",
        "                                ppv += 1\n",
        "                                mean_index += j\n",
        "                                mean += C[j] + _bias\n",
        "                            elif C[j] < _bias:\n",
        "                                stretch = j - last_val\n",
        "                                if stretch > max_stretch:\n",
        "                                    max_stretch = stretch\n",
        "                                last_val = j\n",
        "                        stretch = C.shape[0] - 1 - last_val\n",
        "                        if stretch > max_stretch:\n",
        "                            max_stretch = stretch\n",
        "\n",
        "                        end = feature_index + n_features_per_transform\n",
        "                        features[example_index, end] = ppv / C.shape[0]\n",
        "                        end = end + num_features\n",
        "                        features[example_index, end] = max_stretch\n",
        "                        end = end + num_features\n",
        "                        features[example_index, end] = mean / ppv if ppv > 0 else 0\n",
        "                        end = end + num_features\n",
        "                        features[example_index, end] = mean_index / ppv if ppv > 0 else -1\n",
        "                else:\n",
        "                    _c = C[padding:-padding]\n",
        "\n",
        "                    for feature_count in range(num_features_this_dilation):\n",
        "                        feature_index = feature_index_start + feature_count\n",
        "                        _bias = biases1[feature_index]\n",
        "\n",
        "                        ppv = 0\n",
        "                        last_val = 0\n",
        "                        max_stretch = 0.0\n",
        "                        mean_index = 0\n",
        "                        mean = 0\n",
        "\n",
        "                        for j in range(_c.shape[0]):\n",
        "                            if _c[j] > _bias:\n",
        "                                ppv += 1\n",
        "                                mean_index += j\n",
        "                                mean += _c[j] + _bias\n",
        "                            elif _c[j] < _bias:\n",
        "                                stretch = j - last_val\n",
        "                                if stretch > max_stretch:\n",
        "                                    max_stretch = stretch\n",
        "                                last_val = j\n",
        "                        stretch = _c.shape[0] - 1 - last_val\n",
        "                        if stretch > max_stretch:\n",
        "                            max_stretch = stretch\n",
        "\n",
        "                        end = feature_index + n_features_per_transform\n",
        "                        features[example_index, end] = ppv / _c.shape[0]\n",
        "                        end = end + num_features\n",
        "                        features[example_index, end] = max_stretch\n",
        "                        end = end + num_features\n",
        "                        features[example_index, end] = mean / ppv if ppv > 0 else 0\n",
        "                        end = end + num_features\n",
        "                        features[example_index, end] = mean_index / ppv if ppv > 0 else -1\n",
        "\n",
        "                feature_index_start = feature_index_end\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "class MultiRocket:\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            num_features=50000,\n",
        "            classifier=\"ridge\",\n",
        "            verbose=0\n",
        "    ):\n",
        "        self.name = \"MultiRocket\"\n",
        "\n",
        "        self.base_parameters = None\n",
        "        self.diff1_parameters = None\n",
        "\n",
        "        self.n_features_per_kernel = 4\n",
        "        self.num_features = num_features / 2  # 1 per transformation\n",
        "        self.num_kernels = int(self.num_features / self.n_features_per_kernel)\n",
        "\n",
        "        if verbose > 1:\n",
        "            print('[{}] Creating {} with {} kernels'.format(self.name, self.name, self.num_kernels))\n",
        "\n",
        "        self.clf = classifier\n",
        "        self.classifier = None\n",
        "        self.train_duration = 0\n",
        "        self.test_duration = 0\n",
        "        self.generate_kernel_duration = 0\n",
        "        self.train_transforms_duration = 0\n",
        "        self.test_transforms_duration = 0\n",
        "        self.apply_kernel_on_train_duration = 0\n",
        "        self.apply_kernel_on_test_duration = 0\n",
        "\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def fit(self, x_train, y_train, predict_on_train=True):\n",
        "\n",
        "        if self.verbose > 1:\n",
        "            print('[{}] Training with training set of {}'.format(self.name, x_train.shape))\n",
        "        if x_train.shape[2] < 10:\n",
        "            # handling very short series (like PensDigit from the MTSC archive)\n",
        "            # series have to be at least a length of 10 (including differencing)\n",
        "            _x_train = np.zeros((x_train.shape[0], x_train.shape[1], 10), dtype=x_train.dtype)\n",
        "            _x_train[:, :, :x_train.shape[2]] = x_train\n",
        "            x_train = _x_train\n",
        "            del _x_train\n",
        "\n",
        "        self.generate_kernel_duration = 0\n",
        "        self.apply_kernel_on_train_duration = 0\n",
        "        self.train_transforms_duration = 0\n",
        "\n",
        "        start_time = time.perf_counter()\n",
        "\n",
        "        _start_time = time.perf_counter()\n",
        "        xx = np.diff(x_train, 1)\n",
        "        self.train_transforms_duration += time.perf_counter() - _start_time\n",
        "\n",
        "        _start_time = time.perf_counter()\n",
        "        self.base_parameters = fit(\n",
        "            x_train,\n",
        "            num_features=self.num_kernels\n",
        "        )\n",
        "        self.diff1_parameters = fit(\n",
        "            xx,\n",
        "            num_features=self.num_kernels\n",
        "        )\n",
        "        self.generate_kernel_duration += time.perf_counter() - _start_time\n",
        "\n",
        "        _start_time = time.perf_counter()\n",
        "        x_train_transform = transform(\n",
        "            x_train, xx,\n",
        "            self.base_parameters, self.diff1_parameters,\n",
        "            self.n_features_per_kernel\n",
        "        )\n",
        "        self.apply_kernel_on_train_duration += time.perf_counter() - _start_time\n",
        "\n",
        "        x_train_transform = np.nan_to_num(x_train_transform)\n",
        "\n",
        "        elapsed_time = time.perf_counter() - start_time\n",
        "        if self.verbose > 1:\n",
        "            print('[{}] Kernels applied!, took {}s'.format(self.name, elapsed_time))\n",
        "            print('[{}] Transformed Shape {}'.format(self.name, x_train_transform.shape))\n",
        "\n",
        "        if self.verbose > 1:\n",
        "            print('[{}] Training'.format(self.name))\n",
        "\n",
        "        if self.clf.lower() == \"ridge\":\n",
        "            self.classifier = make_pipeline(\n",
        "                StandardScaler(),\n",
        "                RidgeClassifierCV(\n",
        "                    alphas=np.logspace(-3, 3, 10),\n",
        "                )\n",
        "            )\n",
        "        else:\n",
        "            self.classifier = LogisticRegression(\n",
        "                num_features=x_train_transform.shape[1],\n",
        "                max_epochs=3000,\n",
        "            )\n",
        "        _start_time = time.perf_counter()\n",
        "        self.classifier.fit(x_train_transform, y_train)\n",
        "        self.train_duration = time.perf_counter() - _start_time\n",
        "\n",
        "        if self.verbose > 1:\n",
        "            print('[{}] Training done!, took {:.3f}s'.format(self.name, self.train_duration))\n",
        "        if predict_on_train:\n",
        "            yhat = self.classifier.predict(x_train_transform)\n",
        "        else:\n",
        "            yhat = None\n",
        "\n",
        "        return yhat\n",
        "\n",
        "    def predict(self, x):\n",
        "        if self.verbose > 1:\n",
        "            print('[{}] Predicting'.format(self.name))\n",
        "\n",
        "        self.apply_kernel_on_test_duration = 0\n",
        "        self.test_transforms_duration = 0\n",
        "\n",
        "        _start_time = time.perf_counter()\n",
        "        xx = np.diff(x, 1)\n",
        "        self.test_transforms_duration += time.perf_counter() - _start_time\n",
        "\n",
        "        _start_time = time.perf_counter()\n",
        "        x_transform = transform(\n",
        "            x, xx,\n",
        "            self.base_parameters, self.diff1_parameters,\n",
        "            self.n_features_per_kernel\n",
        "        )\n",
        "        self.apply_kernel_on_test_duration += time.perf_counter() - _start_time\n",
        "\n",
        "        x_transform = np.nan_to_num(x_transform)\n",
        "        if self.verbose > 1:\n",
        "            print('Kernels applied!, took {:.3f}s. Transformed shape: {}.'.format(self.apply_kernel_on_test_duration,\n",
        "                                                                                  x_transform.shape))\n",
        "        start_time = time.perf_counter()\n",
        "        yhat = self.classifier.predict(x_transform)\n",
        "        self.test_duration = time.perf_counter() - start_time\n",
        "        if self.verbose > 1:\n",
        "            print(\"[{}] Predicting completed, took {:.3f}s\".format(self.name, self.test_duration))\n",
        "\n",
        "        return yhat"
      ],
      "metadata": {
        "id": "Siz2i-qNzqhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Construct models"
      ],
      "metadata": {
        "id": "Ede7AE5Iy8Pz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Construct CNN-LSTM model type"
      ],
      "metadata": {
        "id": "kATOhXirzo4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the LSTM model\n",
        "model = lstm(window_size, data.shape[1], 1, 5)"
      ],
      "metadata": {
        "id": "a0zQyv2Ak596"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "15psfCd76-AC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "plot_model(model, show_layer_names = False, dpi=90, show_layer_activations=True, rankdir='TB')"
      ],
      "metadata": {
        "id": "ahr88QIK95jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Construct ResNet"
      ],
      "metadata": {
        "id": "UZzAH8z4zsUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Resnet\n",
        "nb_classes = len(np.unique(np.concatenate((y_train_noisy, y_val), axis=0)))"
      ],
      "metadata": {
        "id": "RAo0iN171Aht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transform the labels from integers to one hot vectors\n",
        "enc = sklearn.preprocessing.OneHotEncoder(categories='auto')\n",
        "enc.fit(np.concatenate((y_train_noisy, y_val), axis=0).reshape(-1, 1))\n",
        "y_train_res = enc.transform(y_train_noisy.reshape(-1, 1)).toarray()\n",
        "y_val_res = enc.transform(y_val.reshape(-1, 1)).toarray()"
      ],
      "metadata": {
        "id": "dMDGhCvy1EqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save orignal y because later we will use binary\n",
        "y_val_true = np.argmax(y_val_res, axis=1)"
      ],
      "metadata": {
        "id": "P5GvKvvr1WyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = x_train.shape[1:]\n",
        "verbose = True\n",
        "output_directory = './drive/MyDrive/kursinis/Modeliai/'\n",
        "\n",
        "classifier = Classifier_RESNET(output_directory, input_shape, nb_classes, verbose)"
      ],
      "metadata": {
        "id": "hdsW6wV1lbKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Construct Regulat LSTM multi\n"
      ],
      "metadata": {
        "id": "C6WnkaAJEkqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "MrCLCD44Epr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Construct FCN (Fully Convolutional Network)"
      ],
      "metadata": {
        "id": "LzulfsAOj1aZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb_classes = len(np.unique(np.concatenate((y_train_noisy, y_val), axis=0)))"
      ],
      "metadata": {
        "id": "D7tYBOayk5jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the LSTM model\n",
        "model = fcn((window_size, data.shape[1]), nb_classes)"
      ],
      "metadata": {
        "id": "JEzPmjTSj7IL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "LIW0e-oulS4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Construct Inception"
      ],
      "metadata": {
        "id": "TYlQ-UNIk5OA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb_classes = len(np.unique(np.concatenate((y_train_noisy, y_val), axis=0)))"
      ],
      "metadata": {
        "id": "mFS3wkXHk7QX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transform the labels from integers to one hot vectors\n",
        "enc = sklearn.preprocessing.OneHotEncoder(categories='auto')\n",
        "enc.fit(np.concatenate((y_train_noisy, y_val), axis=0).reshape(-1, 1))\n",
        "y_train_res = enc.transform(y_train_noisy.reshape(-1, 1)).toarray()\n",
        "y_val_res = enc.transform(y_val.reshape(-1, 1)).toarray()"
      ],
      "metadata": {
        "id": "7RNUo_TOk9zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save orignal y because later we will use binary\n",
        "y_val_true = np.argmax(y_val_res, axis=1)"
      ],
      "metadata": {
        "id": "Z_PVUIg0k9_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = x_train.shape[1:]\n",
        "verbose = True\n",
        "output_directory = './drive/MyDrive/kursinis/Modeliai/'\n",
        "\n",
        "# classifier = Classifier_INCEPTION(output_directory, input_shape, nb_classes, verbose)\n",
        "\n",
        "classifier = Classifier_INCEPTION(\n",
        "    output_directory,\n",
        "    input_shape,\n",
        "    nb_classes,\n",
        "    verbose,\n",
        "    batch_size=32,\n",
        "    nb_filters=32,\n",
        "    depth=6,\n",
        "    kernel_size=41)"
      ],
      "metadata": {
        "id": "kISZjudTk-QL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Construct MLSTM-FCN"
      ],
      "metadata": {
        "id": "NMECRZKVuPxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb_classes = len(np.unique(np.concatenate((y_train_noisy, y_val), axis=0)))"
      ],
      "metadata": {
        "id": "ArxcrymFuR9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the LSTM model\n",
        "model = mlstm_fcn((window_size, data.shape[1]), nb_classes)"
      ],
      "metadata": {
        "id": "xLa6nmA_uUUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "azvwM9JKxIKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Construct MultiRocket"
      ],
      "metadata": {
        "id": "5f38ga6r0YJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import platform\n",
        "import socket\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "import numba\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import psutil\n",
        "import pytz\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sktime.datasets import load_from_tsfile_to_dataframe\n",
        "\n",
        "pd.set_option('display.max_columns', 500)\n",
        "\n",
        "itr = 0\n",
        "num_features = 10000\n",
        "save = True\n",
        "num_threads = 0"
      ],
      "metadata": {
        "id": "D2zRAt4B0aG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_path = os.getcwd() + \"/output/\"\n",
        "classifier_name = \"MultiRocket_{}\".format(num_features)"
      ],
      "metadata": {
        "id": "X09C47bQ0ePM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_classes = len(np.unique(np.concatenate((y_train, y_test), axis=0)))"
      ],
      "metadata": {
        "id": "I2dbneLe86S-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(nb_classes)"
      ],
      "metadata": {
        "id": "Hg37M45X-pHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = x_train.astype(np.float64).copy()\n",
        "X_test = x_test.astype(np.float64).copy()\n",
        "X_val = x_val.astype(np.float64).copy()"
      ],
      "metadata": {
        "id": "DD_AERY7-09j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.transpose(X_train, axes=(0, 2, 1))\n",
        "X_test = np.transpose(X_test, axes=(0, 2, 1))\n",
        "X_val = np.transpose(X_val, axes=(0, 2, 1))\n",
        "\n",
        "\n",
        "X_train.shape, X_val.shape, X_test.shape"
      ],
      "metadata": {
        "id": "DuuYefRPIx84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.perf_counter()\n",
        "\n",
        "output_dir = \"{}/multirocket/resample_{}/{}/\".format(\n",
        "            output_path,\n",
        "            itr,\n",
        "            classifier_name\n",
        "        )"
      ],
      "metadata": {
        "id": "U-iW8ZjH0yeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = MultiRocket(\n",
        "            num_features=50000,\n",
        "            classifier=\"logistic\",\n",
        "            verbose=2\n",
        "        )"
      ],
      "metadata": {
        "id": "roBHPbmT1MHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train model"
      ],
      "metadata": {
        "id": "Us5ANEBY7FkO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train CNN-LSTM"
      ],
      "metadata": {
        "id": "cz3Fzfhp7KGt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train regular"
      ],
      "metadata": {
        "id": "3b4yoDqMYCAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import tensorboard\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "1iENzPPXWef6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_filepath = './drive/MyDrive/kursinis/Modeliai/model_CNN-LSTM-GPT-Strong_300_mul_.{epoch:02d}-{val_loss:.2f}.h5'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    verbose=1,\n",
        "    save_best_only=True)\n",
        "modeL_plateau_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='loss',\n",
        "    factor=0.5,\n",
        "    patience=20,\n",
        "    min_lr=0.0001\n",
        ")"
      ],
      "metadata": {
        "id": "oYwn-3JMgjK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "tensorboard_callback = TensorBoard(\n",
        "    log_dir=log_dir,\n",
        "    histogram_freq=1,\n",
        "    write_graph=True,\n",
        "    write_images=False,\n",
        "    update_freq=\"epoch\",\n",
        ")"
      ],
      "metadata": {
        "id": "drsexr0ZWWkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights('./drive/MyDrive/kursinis/Modeliai/model_CNN-LSTM-GPT-Strong_300_mul_.07-1.38.h5')"
      ],
      "metadata": {
        "id": "ntNeVZkjglub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape the input data to add an extra dimension\n",
        "x_train_noisy = np.expand_dims(x_train_noisy, axis=1)\n",
        "x_train_noisy.shape"
      ],
      "metadata": {
        "id": "6jC_X-7LmyKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_noisy = np.squeeze(x_train_noisy, axis=1)\n",
        "x_train_noisy.shape"
      ],
      "metadata": {
        "id": "Hi0qxklUJnf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_val = np.expand_dims(x_val, axis=1)\n",
        "x_val.shape"
      ],
      "metadata": {
        "id": "lXJ8cM0WpzXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_val = np.squeeze(x_val, axis=1)\n",
        "x_val.shape"
      ],
      "metadata": {
        "id": "fg52VGKNJnzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = np.expand_dims(x_test, axis=1)\n",
        "x_test.shape"
      ],
      "metadata": {
        "id": "gpaL2Ge9ulgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = np.squeeze(x_test, axis=1)\n",
        "x_test.shape"
      ],
      "metadata": {
        "id": "hXtsCnrrJoN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(\n",
        "    x_train_noisy,\n",
        "    y_train_noisy,\n",
        "    validation_data=(x_val, y_val),\n",
        "    epochs=3000,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[model_checkpoint_callback, tensorboard_callback]\n",
        ")"
      ],
      "metadata": {
        "id": "4a8BumIJxjDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir logs"
      ],
      "metadata": {
        "id": "JAizvNrFYI42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train KF Split"
      ],
      "metadata": {
        "id": "dgHGJR2uXop2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv1D, MaxPooling1D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score, classification_report, precision_recall_fscore_support"
      ],
      "metadata": {
        "id": "IB0lGERyZ3rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_splits=5\n",
        "\n",
        "# Initialize cross-validation splitter\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
      ],
      "metadata": {
        "id": "x6o5KKEDZ3rt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "X, y = signals_array, labels_array  # Load your dataset here"
      ],
      "metadata": {
        "id": "lNGhoDepZ3ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_accuracies = []\n",
        "cv_precisions = []\n",
        "cv_recalls = []\n",
        "cv_f1_scores = []\n",
        "all_reports = []"
      ],
      "metadata": {
        "id": "Wrpt9LCNZ3rv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform cross-validation\n",
        "for fold, (train_index, val_index) in enumerate(skf.split(X, y)):\n",
        "    x_train_kf, x_val_kf = X[train_index], X[val_index]\n",
        "    y_train_kf, y_val_kf = y[train_index], y[val_index]\n",
        "\n",
        "    print(x_train_kf.shape)\n",
        "    print(x_val_kf.shape)\n",
        "\n",
        "    # Compute mean and standard deviation over the training set\n",
        "    mu = np.mean(x_train_kf)\n",
        "    va = np.std(x_train_kf)\n",
        "\n",
        "    print(mu)\n",
        "    print(va)\n",
        "\n",
        "    x_train_kf = (x_train_kf - mu) / va\n",
        "    x_val_kf = (x_val_kf - mu) / va\n",
        "\n",
        "    print(np.mean(x_train_kf))\n",
        "    print(np.std(x_train_kf))\n",
        "    print(np.mean(x_val_kf))\n",
        "    print(np.std(x_val_kf))\n",
        "\n",
        "    x_train_kf = np.expand_dims(x_train_kf, axis=1)\n",
        "    x_val_kf = np.expand_dims(x_val_kf, axis=1)\n",
        "\n",
        "    class_weights = compute_class_weight(\n",
        "                                            class_weight = \"balanced\",\n",
        "                                            classes = np.unique(y_train_kf),\n",
        "                                            y = y_train_kf\n",
        "                                        )\n",
        "    class_weights = dict(zip(np.unique(y_train_kf), class_weights))\n",
        "    class_weights\n",
        "\n",
        "    nb_classes = len(np.unique(np.concatenate((y_train_kf, y_val_kf), axis=0)))\n",
        "\n",
        "    # Create the LSTM model\n",
        "    model = lstm(window_size, data.shape[1], 1, nb_classes)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Create and compile FCN model\n",
        "    model.fit(\n",
        "        x_train_kf,\n",
        "        y_train_kf,\n",
        "        validation_data=(x_val_kf, y_val_kf),\n",
        "        epochs=2,\n",
        "        batch_size=32,\n",
        "        shuffle=True,\n",
        "        class_weight=class_weights,\n",
        "        callbacks=[model_checkpoint_callback, modeL_plateau_callback]\n",
        "    )\n",
        "\n",
        "    # Evaluate the model and calculate accuracy\n",
        "    predictions = model.predict(x_val_kf)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_val_kf, np.argmax(predictions, axis=1))\n",
        "\n",
        "    # Calculate precision, recall, and F1 score\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_val_kf, np.argmax(predictions, axis=1), average='weighted')\n",
        "\n",
        "    accuracy *= 100\n",
        "    precision *= 100\n",
        "    recall *= 100\n",
        "    f1 *= 100\n",
        "\n",
        "    # Generate classification report\n",
        "    report = classification_report(y_val_kf, np.argmax(predictions, axis=1))\n",
        "\n",
        "    # Append scores to lists\n",
        "    all_reports.append(report)\n",
        "    cv_accuracies.append(accuracy)\n",
        "    cv_precisions.append(precision)\n",
        "    cv_recalls.append(recall)\n",
        "    cv_f1_scores.append(f1)\n",
        "\n",
        "    # Print results for this fold\n",
        "    print(f\"Accuracy for fold {fold + 1}: {accuracy:.2f}%\")\n",
        "    print(f\"Precision for fold {fold + 1}: {precision:.2f}%\")\n",
        "    print(f\"Recall for fold {fold + 1}: {recall:.2f}%\")\n",
        "    print(f\"F1Score for fold {fold + 1}: {f1:.2f}%\")\n",
        "    print(report)\n",
        "    print()"
      ],
      "metadata": {
        "id": "LVPj2kMPZ3rv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for fold in range(n_splits):\n",
        "    print(f\"Fold {fold + 1}:\")\n",
        "    print(\"Accuracy:\", cv_accuracies[fold])\n",
        "    print(\"Classification report:\")\n",
        "    print(all_reports[fold])\n",
        "    print()\n",
        "\n",
        "print(\"Average Accuracy:\", np.mean(cv_accuracies))\n",
        "print(\"Average Precision:\", np.mean(cv_precisions))\n",
        "print(\"Average Recall:\", np.mean(cv_recalls))\n",
        "print(\"Average F1-score:\", np.mean(cv_f1_scores))"
      ],
      "metadata": {
        "id": "HIN9TzG1Z3rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate average accuracy and standard deviation\n",
        "average_accuracy = np.mean(cv_accuracies)\n",
        "std_dev_accuracy = np.std(cv_accuracies)\n",
        "\n",
        "average_precision = np.mean(cv_precisions)\n",
        "std_dev_precision = np.std(cv_precisions)\n",
        "\n",
        "average_recall = np.mean(cv_recalls)\n",
        "std_dev_recall = np.std(cv_recalls)\n",
        "\n",
        "average_f1_scores = np.mean(cv_f1_scores)\n",
        "std_dev_f1_scores = np.std(cv_f1_scores)\n",
        "\n",
        "# Calculate error rate\n",
        "error_rate_accuracy = std_dev_accuracy / np.sqrt(len(cv_accuracies))\n",
        "error_rate_precision = std_dev_precision / np.sqrt(len(cv_precisions))\n",
        "error_rate_recall = std_dev_recall / np.sqrt(len(cv_recalls))\n",
        "error_rate_f1_score = std_dev_f1_scores / np.sqrt(len(cv_f1_scores))\n",
        "\n",
        "\n",
        "# Print results\n",
        "print(f\"Average Accuracy: {average_accuracy:.2f}% +- {error_rate_accuracy:.2f}%\")\n",
        "print(f\"Average Precision: {average_precision:.2f}% +- {error_rate_precision:.2f}%\")\n",
        "print(f\"Average Recall: {average_recall:.2f}% +- {error_rate_recall:.2f}%\")\n",
        "print(f\"Average F1 score: {average_f1_scores:.2f}% +- {error_rate_f1_score:.2f}%\")"
      ],
      "metadata": {
        "id": "KTiamgJlZ3rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train ResNet"
      ],
      "metadata": {
        "id": "EkOncaha7Mp1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train Regular"
      ],
      "metadata": {
        "id": "SAktcAqNi5Ua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.fit(\n",
        "    x_train_noisy,\n",
        "    y_train_res,\n",
        "    x_val,\n",
        "    y_val_res,\n",
        "    y_val_true,\n",
        "    batch_size = 32,\n",
        "    nb_epochs = 3000,\n",
        "    shuffle=True,\n",
        "    class_weights=class_weights)"
      ],
      "metadata": {
        "id": "AC0GZDHb7SRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train KF split"
      ],
      "metadata": {
        "id": "kfODMC9uUL0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv1D, MaxPooling1D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score, classification_report, precision_recall_fscore_support"
      ],
      "metadata": {
        "id": "cAj7yYRcUXPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_splits=5\n",
        "\n",
        "# Initialize cross-validation splitter\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
      ],
      "metadata": {
        "id": "T8D-PR_HUXPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "X, y = signals_array, labels_array  # Load your dataset here"
      ],
      "metadata": {
        "id": "lnmh2EJJUXPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_accuracies = []\n",
        "cv_precisions = []\n",
        "cv_recalls = []\n",
        "cv_f1_scores = []\n",
        "all_reports = []"
      ],
      "metadata": {
        "id": "em42znM0UXPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform cross-validation\n",
        "for fold, (train_index, val_index) in enumerate(skf.split(X, y)):\n",
        "    x_train_kf, x_val_kf = X[train_index], X[val_index]\n",
        "    y_train_kf, y_val_kf = y[train_index], y[val_index]\n",
        "\n",
        "    print(x_train_kf.shape)\n",
        "    print(x_val_kf.shape)\n",
        "\n",
        "    # Compute mean and standard deviation over the training set\n",
        "    mu = np.mean(x_train_kf)\n",
        "    va = np.std(x_train_kf)\n",
        "\n",
        "    print(mu)\n",
        "    print(va)\n",
        "\n",
        "    x_train_kf = (x_train_kf - mu) / va\n",
        "    x_val_kf = (x_val_kf - mu) / va\n",
        "\n",
        "    print(np.mean(x_train_kf))\n",
        "    print(np.std(x_train_kf))\n",
        "    print(np.mean(x_val_kf))\n",
        "    print(np.std(x_val_kf))\n",
        "\n",
        "    class_weights = compute_class_weight(\n",
        "                                            class_weight = \"balanced\",\n",
        "                                            classes = np.unique(y_train_kf),\n",
        "                                            y = y_train_kf\n",
        "                                        )\n",
        "    class_weights = dict(zip(np.unique(y_train_kf), class_weights))\n",
        "    class_weights\n",
        "\n",
        "    nb_classes = len(np.unique(np.concatenate((y_train_kf, y_val_kf), axis=0)))\n",
        "\n",
        "    x_train_kf, y_train_kf = add_laplace_noise_to_array(x_train_kf, y_train_kf, 0.05, 12)\n",
        "\n",
        "    print(x_train_kf.shape)\n",
        "    print(y_train_kf.shape)\n",
        "\n",
        "    # transform the labels from integers to one hot vectors\n",
        "    enc = sklearn.preprocessing.OneHotEncoder(categories='auto')\n",
        "    enc.fit(np.concatenate((y_train_kf, y_val_kf), axis=0).reshape(-1, 1))\n",
        "    y_train_res = enc.transform(y_train_kf.reshape(-1, 1)).toarray()\n",
        "    y_val_res = enc.transform(y_val_kf.reshape(-1, 1)).toarray()\n",
        "\n",
        "    # save orignal y because later we will use binary\n",
        "    y_val_true = np.argmax(y_val_res, axis=1)\n",
        "\n",
        "    input_shape = x_train_kf.shape[1:]\n",
        "    verbose = True\n",
        "    output_directory = './drive/MyDrive/kursinis/Modeliai/'\n",
        "\n",
        "    classifier = Classifier_RESNET(output_directory, input_shape, nb_classes, verbose)\n",
        "\n",
        "    classifier.fit(\n",
        "        x_train_kf,\n",
        "        y_train_res,\n",
        "        x_val_kf,\n",
        "        y_val_res,\n",
        "        y_val_true,\n",
        "        batch_size = 16,\n",
        "        nb_epochs = 150,\n",
        "        shuffle=True,\n",
        "        class_weights=class_weights)\n",
        "\n",
        "    # Evaluate the model and calculate accuracy\n",
        "    predictions = classifier.model.predict(x_val_kf)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_val_kf, np.argmax(predictions, axis=1))\n",
        "\n",
        "    # Calculate precision, recall, and F1 score\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_val_kf, np.argmax(predictions, axis=1), average='weighted')\n",
        "\n",
        "    accuracy *= 100\n",
        "    precision *= 100\n",
        "    recall *= 100\n",
        "    f1 *= 100\n",
        "\n",
        "    report = classification_report(y_val_kf, np.argmax(predictions, axis=1))\n",
        "\n",
        "    # Append scores to lists\n",
        "    all_reports.append(report)\n",
        "    cv_accuracies.append(accuracy)\n",
        "    cv_precisions.append(precision)\n",
        "    cv_recalls.append(recall)\n",
        "    cv_f1_scores.append(f1)\n",
        "\n",
        "    # Print results for this fold\n",
        "    print(f\"Accuracy for fold {fold + 1}: {accuracy:.2f}%\")\n",
        "    print(f\"Precision for fold {fold + 1}: {precision:.2f}%\")\n",
        "    print(f\"Recall for fold {fold + 1}: {recall:.2f}%\")\n",
        "    print(f\"F1Score for fold {fold + 1}: {f1:.2f}%\")\n",
        "    print(report)\n",
        "    print()"
      ],
      "metadata": {
        "id": "vHEcgGg0UXPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for fold in range(n_splits):\n",
        "    print(f\"Fold {fold + 1}:\")\n",
        "    print(\"Accuracy:\", cv_accuracies[fold])\n",
        "    print(\"Classification report:\")\n",
        "    print(all_reports[fold])\n",
        "    print()\n",
        "\n",
        "print(\"Average Accuracy:\", np.mean(cv_accuracies))\n",
        "print(\"Average Precision:\", np.mean(cv_precisions))\n",
        "print(\"Average Recall:\", np.mean(cv_recalls))\n",
        "print(\"Average F1-score:\", np.mean(cv_f1_scores))"
      ],
      "metadata": {
        "id": "KvQvcsYkUXPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate average accuracy and standard deviation\n",
        "average_accuracy = np.mean(cv_accuracies)\n",
        "std_dev_accuracy = np.std(cv_accuracies)\n",
        "\n",
        "average_precision = np.mean(cv_precisions)\n",
        "std_dev_precision = np.std(cv_precisions)\n",
        "\n",
        "average_recall = np.mean(cv_recalls)\n",
        "std_dev_recall = np.std(cv_recalls)\n",
        "\n",
        "average_f1_scores = np.mean(cv_f1_scores)\n",
        "std_dev_f1_scores = np.std(cv_f1_scores)\n",
        "\n",
        "# Calculate error rate\n",
        "error_rate_accuracy = std_dev_accuracy / np.sqrt(len(cv_accuracies))\n",
        "error_rate_precision = std_dev_precision / np.sqrt(len(cv_precisions))\n",
        "error_rate_recall = std_dev_recall / np.sqrt(len(cv_recalls))\n",
        "error_rate_f1_score = std_dev_f1_scores / np.sqrt(len(cv_f1_scores))\n",
        "\n",
        "\n",
        "# Print results\n",
        "print(f\"Average Accuracy: {average_accuracy:.2f}% +- {error_rate_accuracy:.2f}%\")\n",
        "print(f\"Average Precision: {average_precision:.2f}% +- {error_rate_precision:.2f}%\")\n",
        "print(f\"Average Recall: {average_recall:.2f}% +- {error_rate_recall:.2f}%\")\n",
        "print(f\"Average F1 score: {average_f1_scores:.2f}% +- {error_rate_f1_score:.2f}%\")"
      ],
      "metadata": {
        "id": "-ELuY5POUXPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train FCN"
      ],
      "metadata": {
        "id": "ilySMcEQlbvb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_filepath = './drive/MyDrive/kursinis/Modeliai/model_FCN_.{epoch:02d}-{val_loss:.2f}.h5'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    verbose=1,\n",
        "    save_best_only=True)\n",
        "\n",
        "modeL_plateau_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='loss',\n",
        "    factor=0.5,\n",
        "    patience=50,\n",
        "    min_lr=0.0001\n",
        ")"
      ],
      "metadata": {
        "id": "U-cJy1rKldmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    x_train_noisy,\n",
        "    y_train_noisy,\n",
        "    validation_data=(x_val, y_val),\n",
        "    epochs=3000,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[model_checkpoint_callback, modeL_plateau_callback]\n",
        ")"
      ],
      "metadata": {
        "id": "6S4S1fWWltJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train KF Split"
      ],
      "metadata": {
        "id": "lqHsGUDbuXiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv1D, MaxPooling1D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score, classification_report, precision_recall_fscore_support"
      ],
      "metadata": {
        "id": "D0xTk1_WuZkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_splits=5\n",
        "\n",
        "# Initialize cross-validation splitter\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
      ],
      "metadata": {
        "id": "8EI3XwMaulvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "X, y = signals_array, labels_array  # Load your dataset here"
      ],
      "metadata": {
        "id": "G-gfArIEvdJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_accuracies = []\n",
        "cv_precisions = []\n",
        "cv_recalls = []\n",
        "cv_f1_scores = []\n",
        "all_reports = []"
      ],
      "metadata": {
        "id": "VWS9YOpk5is-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform cross-validation\n",
        "for fold, (train_index, val_index) in enumerate(skf.split(X, y)):\n",
        "    x_train_kf, x_val_kf = X[train_index], X[val_index]\n",
        "    y_train_kf, y_val_kf = y[train_index], y[val_index]\n",
        "\n",
        "    print(x_train_kf.shape)\n",
        "    print(x_val_kf.shape)\n",
        "\n",
        "    # Compute mean and standard deviation over the training set\n",
        "    mu = np.mean(x_train_kf)\n",
        "    va = np.std(x_train_kf)\n",
        "\n",
        "    print(mu)\n",
        "    print(va)\n",
        "\n",
        "    x_train_kf = (x_train_kf - mu) / va\n",
        "    x_val_kf = (x_val_kf - mu) / va\n",
        "\n",
        "    print(np.mean(x_train_kf))\n",
        "    print(np.std(x_train_kf))\n",
        "    print(np.mean(x_val_kf))\n",
        "    print(np.std(x_val_kf))\n",
        "\n",
        "    class_weights = compute_class_weight(\n",
        "                                            class_weight = \"balanced\",\n",
        "                                            classes = np.unique(y_train_kf),\n",
        "                                            y = y_train_kf\n",
        "                                        )\n",
        "    class_weights = dict(zip(np.unique(y_train_kf), class_weights))\n",
        "    class_weights\n",
        "\n",
        "    nb_classes = len(np.unique(np.concatenate((y_train_kf, y_val_kf), axis=0)))\n",
        "\n",
        "    # Create the LSTM model\n",
        "    model = fcn((window_size, data.shape[1]), nb_classes)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Create and compile FCN model\n",
        "    model.fit(\n",
        "        x_train_kf,\n",
        "        y_train_kf,\n",
        "        validation_data=(x_val_kf, y_val_kf),\n",
        "        epochs=2,\n",
        "        batch_size=32,\n",
        "        shuffle=True,\n",
        "        class_weight=class_weights,\n",
        "        callbacks=[model_checkpoint_callback, modeL_plateau_callback]\n",
        "    )\n",
        "\n",
        "    # Evaluate the model and calculate accuracy\n",
        "    predictions = model.predict(x_val_kf)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_val_kf, np.argmax(predictions, axis=1))\n",
        "\n",
        "    # Calculate precision, recall, and F1 score\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_val_kf, np.argmax(predictions, axis=1), average='weighted')\n",
        "\n",
        "    accuracy *= 100\n",
        "    precision *= 100\n",
        "    recall *= 100\n",
        "    f1 *= 100\n",
        "\n",
        "    # Generate classification report\n",
        "    report = classification_report(y_val_kf, np.argmax(predictions, axis=1))\n",
        "\n",
        "    # Append scores to lists\n",
        "    all_reports.append(report)\n",
        "    cv_accuracies.append(accuracy)\n",
        "    cv_precisions.append(precision)\n",
        "    cv_recalls.append(recall)\n",
        "    cv_f1_scores.append(f1)\n",
        "\n",
        "    # Print results for this fold\n",
        "    print(f\"Accuracy for fold {fold + 1}: {accuracy:.2f}%\")\n",
        "    print(f\"Precision for fold {fold + 1}: {precision:.2f}%\")\n",
        "    print(f\"Recall for fold {fold + 1}: {recall:.2f}%\")\n",
        "    print(f\"F1Score for fold {fold + 1}: {f1:.2f}%\")\n",
        "    print(report)\n",
        "    print()"
      ],
      "metadata": {
        "id": "hNSeQ1LGup6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for fold in range(n_splits):\n",
        "    print(f\"Fold {fold + 1}:\")\n",
        "    print(\"Accuracy:\", cv_accuracies[fold])\n",
        "    print(\"Classification report:\")\n",
        "    print(all_reports[fold])\n",
        "    print()\n",
        "\n",
        "print(\"Average Accuracy:\", np.mean(cv_accuracies))\n",
        "print(\"Average Precision:\", np.mean(cv_precisions))\n",
        "print(\"Average Recall:\", np.mean(cv_recalls))\n",
        "print(\"Average F1-score:\", np.mean(cv_f1_scores))"
      ],
      "metadata": {
        "id": "3hjCygno51KH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate average accuracy and standard deviation\n",
        "average_accuracy = np.mean(cv_accuracies)\n",
        "std_dev_accuracy = np.std(cv_accuracies)\n",
        "\n",
        "average_precision = np.mean(cv_precisions)\n",
        "std_dev_precision = np.std(cv_precisions)\n",
        "\n",
        "average_recall = np.mean(cv_recalls)\n",
        "std_dev_recall = np.std(cv_recalls)\n",
        "\n",
        "average_f1_scores = np.mean(cv_f1_scores)\n",
        "std_dev_f1_scores = np.std(cv_f1_scores)\n",
        "\n",
        "# Calculate error rate\n",
        "error_rate_accuracy = std_dev_accuracy / np.sqrt(len(cv_accuracies))\n",
        "error_rate_precision = std_dev_precision / np.sqrt(len(cv_precisions))\n",
        "error_rate_recall = std_dev_recall / np.sqrt(len(cv_recalls))\n",
        "error_rate_f1_score = std_dev_f1_scores / np.sqrt(len(cv_f1_scores))\n",
        "\n",
        "\n",
        "# Print results\n",
        "print(f\"Average Accuracy: {average_accuracy:.2f}% +- {error_rate_accuracy:.2f}%\")\n",
        "print(f\"Average Precision: {average_precision:.2f}% +- {error_rate_precision:.2f}%\")\n",
        "print(f\"Average Recall: {average_recall:.2f}% +- {error_rate_recall:.2f}%\")\n",
        "print(f\"Average F1 score: {average_f1_scores:.2f}% +- {error_rate_f1_score:.2f}%\")"
      ],
      "metadata": {
        "id": "WbPEb9-J8JT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Inception"
      ],
      "metadata": {
        "id": "M7QCvMoomRK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.fit(\n",
        "    x_train_noisy,\n",
        "    y_train_res,\n",
        "    x_val,\n",
        "    y_val_res,\n",
        "    y_val_true,\n",
        "    batch_size = 32,\n",
        "    nb_epochs = 3000,\n",
        "    shuffle=True,\n",
        "    class_weights=class_weights)"
      ],
      "metadata": {
        "id": "XaGaTTwRmVt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train KF Split"
      ],
      "metadata": {
        "id": "N68a_csiHYm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv1D, MaxPooling1D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score, classification_report, precision_recall_fscore_support"
      ],
      "metadata": {
        "id": "ti_gquU2HblI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_splits=5\n",
        "\n",
        "# Initialize cross-validation splitter\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
      ],
      "metadata": {
        "id": "gfMuZ5pzHfXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "X, y = signals_array, labels_array  # Load your dataset here"
      ],
      "metadata": {
        "id": "feJbj6EqHjxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_accuracies = []\n",
        "cv_precisions = []\n",
        "cv_recalls = []\n",
        "cv_f1_scores = []\n",
        "all_reports = []"
      ],
      "metadata": {
        "id": "vr66DmXQHkP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform cross-validation\n",
        "for fold, (train_index, val_index) in enumerate(skf.split(X, y)):\n",
        "    x_train_kf, x_val_kf = X[train_index], X[val_index]\n",
        "    y_train_kf, y_val_kf = y[train_index], y[val_index]\n",
        "\n",
        "    print(x_train_kf.shape)\n",
        "    print(x_val_kf.shape)\n",
        "\n",
        "    # Compute mean and standard deviation over the training set\n",
        "    mu = np.mean(x_train_kf)\n",
        "    va = np.std(x_train_kf)\n",
        "\n",
        "    print(mu)\n",
        "    print(va)\n",
        "\n",
        "    x_train_kf = (x_train_kf - mu) / va\n",
        "    x_val_kf = (x_val_kf - mu) / va\n",
        "\n",
        "    print(np.mean(x_train_kf))\n",
        "    print(np.std(x_train_kf))\n",
        "    print(np.mean(x_val_kf))\n",
        "    print(np.std(x_val_kf))\n",
        "\n",
        "    class_weights = compute_class_weight(\n",
        "                                            class_weight = \"balanced\",\n",
        "                                            classes = np.unique(y_train_kf),\n",
        "                                            y = y_train_kf\n",
        "                                        )\n",
        "    class_weights = dict(zip(np.unique(y_train_kf), class_weights))\n",
        "    class_weights\n",
        "\n",
        "    nb_classes = len(np.unique(np.concatenate((y_train_kf, y_val_kf), axis=0)))\n",
        "\n",
        "    # transform the labels from integers to one hot vectors\n",
        "    enc = sklearn.preprocessing.OneHotEncoder(categories='auto')\n",
        "    enc.fit(np.concatenate((y_train_kf, y_val_kf), axis=0).reshape(-1, 1))\n",
        "    y_train_res = enc.transform(y_train_kf.reshape(-1, 1)).toarray()\n",
        "    y_val_res = enc.transform(y_val_kf.reshape(-1, 1)).toarray()\n",
        "\n",
        "    # save orignal y because later we will use binary\n",
        "    y_val_true = np.argmax(y_val_res, axis=1)\n",
        "\n",
        "    input_shape = x_train_kf.shape[1:]\n",
        "    verbose = True\n",
        "    output_directory = './drive/MyDrive/kursinis/Modeliai/'\n",
        "\n",
        "    # classifier = Classifier_INCEPTION(output_directory, input_shape, nb_classes, verbose)\n",
        "\n",
        "    classifier = Classifier_INCEPTION(\n",
        "        output_directory,\n",
        "        input_shape,\n",
        "        nb_classes,\n",
        "        verbose,\n",
        "        batch_size=32,\n",
        "        nb_filters=32,\n",
        "        depth=6,\n",
        "        kernel_size=41)\n",
        "\n",
        "    classifier.fit(\n",
        "        x_train_kf,\n",
        "        y_train_res,\n",
        "        x_val_kf,\n",
        "        y_val_res,\n",
        "        y_val_true,\n",
        "        batch_size = 32,\n",
        "        nb_epochs = 1500,\n",
        "        shuffle=True,\n",
        "        class_weights=class_weights)\n",
        "\n",
        "    # Evaluate the model and calculate accuracy\n",
        "    predictions = classifier.model.predict(x_val_kf)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_val_kf, np.argmax(predictions, axis=1))\n",
        "\n",
        "    # Calculate precision, recall, and F1 score\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_val_kf, np.argmax(predictions, axis=1), average='weighted')\n",
        "\n",
        "    accuracy *= 100\n",
        "    precision *= 100\n",
        "    recall *= 100\n",
        "    f1 *= 100\n",
        "\n",
        "    report = classification_report(y_val_kf, np.argmax(predictions, axis=1))\n",
        "\n",
        "    # Append scores to lists\n",
        "    all_reports.append(report)\n",
        "    cv_accuracies.append(accuracy)\n",
        "    cv_precisions.append(precision)\n",
        "    cv_recalls.append(recall)\n",
        "    cv_f1_scores.append(f1)\n",
        "\n",
        "    # Print results for this fold\n",
        "    print(f\"Accuracy for fold {fold + 1}: {accuracy:.2f}%\")\n",
        "    print(f\"Precision for fold {fold + 1}: {precision:.2f}%\")\n",
        "    print(f\"Recall for fold {fold + 1}: {recall:.2f}%\")\n",
        "    print(f\"F1Score for fold {fold + 1}: {f1:.2f}%\")\n",
        "    print(report)\n",
        "    print()"
      ],
      "metadata": {
        "id": "JW0FsGbYHmn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for fold in range(n_splits):\n",
        "    print(f\"Fold {fold + 1}:\")\n",
        "    print(\"Accuracy:\", cv_accuracies[fold])\n",
        "    print(\"Classification report:\")\n",
        "    print(all_reports[fold])\n",
        "    print()\n",
        "\n",
        "print(\"Average Accuracy:\", np.mean(cv_accuracies))\n",
        "print(\"Average Precision:\", np.mean(cv_precisions))\n",
        "print(\"Average Recall:\", np.mean(cv_recalls))\n",
        "print(\"Average F1-score:\", np.mean(cv_f1_scores))"
      ],
      "metadata": {
        "id": "R1qRBRMILECt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate average accuracy and standard deviation\n",
        "average_accuracy = np.mean(cv_accuracies)\n",
        "std_dev_accuracy = np.std(cv_accuracies)\n",
        "\n",
        "average_precision = np.mean(cv_precisions)\n",
        "std_dev_precision = np.std(cv_precisions)\n",
        "\n",
        "average_recall = np.mean(cv_recalls)\n",
        "std_dev_recall = np.std(cv_recalls)\n",
        "\n",
        "average_f1_scores = np.mean(cv_f1_scores)\n",
        "std_dev_f1_scores = np.std(cv_f1_scores)\n",
        "\n",
        "# Calculate error rate\n",
        "error_rate_accuracy = std_dev_accuracy / np.sqrt(len(cv_accuracies))\n",
        "error_rate_precision = std_dev_precision / np.sqrt(len(cv_precisions))\n",
        "error_rate_recall = std_dev_recall / np.sqrt(len(cv_recalls))\n",
        "error_rate_f1_score = std_dev_f1_scores / np.sqrt(len(cv_f1_scores))\n",
        "\n",
        "\n",
        "# Print results\n",
        "print(f\"Average Accuracy: {average_accuracy:.2f}% +- {error_rate_accuracy:.2f}%\")\n",
        "print(f\"Average Precision: {average_precision:.2f}% +- {error_rate_precision:.2f}%\")\n",
        "print(f\"Average Recall: {average_recall:.2f}% +- {error_rate_recall:.2f}%\")\n",
        "print(f\"Average F1 score: {average_f1_scores:.2f}% +- {error_rate_f1_score:.2f}%\")"
      ],
      "metadata": {
        "id": "vuGaHcJ8LJCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train MLSTM-FCN"
      ],
      "metadata": {
        "id": "AZjNXCEtxSd5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_filepath = './drive/MyDrive/kursinis/Modeliai/model_MLSTM-FCN_.{epoch:02d}-{val_loss:.2f}.h5'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_loss',\n",
        "    verbose=1,\n",
        "    save_best_only=True)\n",
        "\n",
        "modeL_plateau_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='loss',\n",
        "    factor=0.5,\n",
        "    patience=50,\n",
        "    min_lr=0.0001\n",
        ")"
      ],
      "metadata": {
        "id": "70KrGEh7xUlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    x_train_noisy,\n",
        "    y_train_noisy,\n",
        "    validation_data=(x_val, y_val),\n",
        "    epochs=3000,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[model_checkpoint_callback, modeL_plateau_callback]\n",
        ")"
      ],
      "metadata": {
        "id": "SF82TO0gxxTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train MultiRocket"
      ],
      "metadata": {
        "id": "BiStNrCppyKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yhat_train = classifier.fit(\n",
        "            X_train, y_train,\n",
        "            predict_on_train=True\n",
        "        )"
      ],
      "metadata": {
        "id": "UAy8ih9Jp0J0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if yhat_train is not None:\n",
        "    train_acc = accuracy_score(y_train, yhat_train)\n",
        "else:\n",
        "    train_acc = -1"
      ],
      "metadata": {
        "id": "YCR2diLDp60Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate model"
      ],
      "metadata": {
        "id": "22ZHHeXn7QhB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate CNN-LSTM"
      ],
      "metadata": {
        "id": "Q31uWuk57iQ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Softmax"
      ],
      "metadata": {
        "id": "JI974neTcbme"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights('./drive/MyDrive/kursinis/Modeliai/model_LSTM_.2452-0.96.h5')"
      ],
      "metadata": {
        "id": "BeIHwSC-Dybi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "model.evaluate(x_test, y_test, verbose=1)"
      ],
      "metadata": {
        "id": "Y7Kv6cvxVMds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model and calculate accuracy\n",
        "predictions = model.predict(x_test)"
      ],
      "metadata": {
        "id": "xHMNn2II3iyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(predictions.shape)\n",
        "print(x_test.shape)\n",
        "\n",
        "# Calculate accuracy for each label\n",
        "rounded_predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Calculate accuracy for each label\n",
        "correct = np.sum(rounded_predictions == y_test)\n",
        "\n",
        "total_samples = len(y_test)\n",
        "accuracy = correct / total_samples * 100\n",
        "print(f\"Overall Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "hvFuLmZN3iPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "unique_labels, counts = np.unique(labels_array, return_counts=True)\n",
        "print(\"Window size:\", window_size)\n",
        "print(\"Step size:\", step_size)\n",
        "print()\n",
        "print(\"Unique Labels:\", unique_labels)\n",
        "print(\"Class counts:\", counts)\n",
        "print(\"Original data:\", signals_array.shape)\n",
        "print()\n",
        "print(\"Train data (No aug):\", x_train.shape)\n",
        "print(\"Val data (No aug):\", x_val.shape)\n",
        "print(\"Test data (No aug):\", x_test.shape)\n",
        "print()\n",
        "print(\"Train data augmented:\", x_train_noisy.shape)\n",
        "print()\n",
        "print(f\"Test set accuracy: {accuracy:.2f}%\")\n",
        "print()\n",
        "print(classification_report(y_test, rounded_predictions))"
      ],
      "metadata": {
        "id": "zDy0fHDIKTHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate ResNet"
      ],
      "metadata": {
        "id": "WIyKN57k7mF-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Softmax"
      ],
      "metadata": {
        "id": "H-TJrb4qhc2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load best model\n",
        "model_path = classifier.output_directory + 'best_model.498-1.19.h5'\n",
        "model = keras.models.load_model(model_path)\n",
        "\n",
        "start_time = time.time()\n",
        "y_pred = model.predict(x_test)"
      ],
      "metadata": {
        "id": "0GZUJxRkUHlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "y_pred = classifier.model.predict(x_test)"
      ],
      "metadata": {
        "id": "AsB7AhX87uS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_pred.shape)\n",
        "print(x_test.shape)\n",
        "\n",
        "# Calculate accuracy for each label\n",
        "rounded_predictions = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Calculate accuracy for each label\n",
        "correct = np.sum(rounded_predictions == y_test)\n",
        "\n",
        "total_samples = len(y_test)\n",
        "accuracy = correct / total_samples * 100\n",
        "print(f\"Overall Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "x8DNbhIW8CpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "unique_labels, counts = np.unique(labels_array, return_counts=True)\n",
        "print(\"Window size:\", window_size)\n",
        "print(\"Step size:\", step_size)\n",
        "print()\n",
        "print(\"Unique Labels:\", unique_labels)\n",
        "print(\"Class counts:\", counts)\n",
        "print(\"Original data:\", signals_array.shape)\n",
        "print()\n",
        "print(\"Train data (No aug):\", x_train.shape)\n",
        "print(\"Val data (No aug):\", x_val.shape)\n",
        "print(\"Test data (No aug):\", x_test.shape)\n",
        "print()\n",
        "# print(\"Train data augmented:\", x_train_noisy.shape)\n",
        "print()\n",
        "print(f\"Test set accuracy: {accuracy:.2f}%\")\n",
        "print()\n",
        "print(classification_report(y_test, rounded_predictions))"
      ],
      "metadata": {
        "id": "bBHVf5Nu8Q1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate FCN"
      ],
      "metadata": {
        "id": "wnBsD3dw2WH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights('./drive/MyDrive/kursinis/Modeliai/model_FCN_.479-0.73.h5')"
      ],
      "metadata": {
        "id": "K7MiWdvh209n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "model.evaluate(x_test, y_test, verbose=1)"
      ],
      "metadata": {
        "id": "PKdzKyRh2Rl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model and calculate accuracy\n",
        "predictions = model.predict(x_test)"
      ],
      "metadata": {
        "id": "qDmDRkZx2ldN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(predictions.shape)\n",
        "print(x_test.shape)\n",
        "\n",
        "# Calculate accuracy for each label\n",
        "rounded_predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Calculate accuracy for each label\n",
        "correct = np.sum(rounded_predictions == y_test)\n",
        "\n",
        "total_samples = len(y_test)\n",
        "accuracy = correct / total_samples * 100\n",
        "print(f\"Overall Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "apSPy_9e2nUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "unique_labels, counts = np.unique(labels_array, return_counts=True)\n",
        "print(\"Window size:\", window_size)\n",
        "print(\"Step size:\", step_size)\n",
        "print()\n",
        "print(\"Unique Labels:\", unique_labels)\n",
        "print(\"Class counts:\", counts)\n",
        "print(\"Original data:\", signals_array.shape)\n",
        "print()\n",
        "print(\"Train data (No aug):\", x_train.shape)\n",
        "print(\"Val data (No aug):\", x_val.shape)\n",
        "print(\"Test data (No aug):\", x_test.shape)\n",
        "print()\n",
        "print(\"Train data augmented:\", x_train_noisy.shape)\n",
        "print()\n",
        "print(f\"Test set accuracy: {accuracy:.2f}%\")\n",
        "print()\n",
        "print(classification_report(y_test, rounded_predictions))"
      ],
      "metadata": {
        "id": "SqXUcnfn2oyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate MultiRocket"
      ],
      "metadata": {
        "id": "cbYqNTK0p79t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yhat_test = classifier.predict(X_test)\n",
        "test_acc = accuracy_score(y_test, yhat_test)"
      ],
      "metadata": {
        "id": "uDhESfuhp9nY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yhat_val = classifier.predict(X_val)\n",
        "val_acc = accuracy_score(y_val, yhat_val)"
      ],
      "metadata": {
        "id": "5Zul8ff3qCEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_acc)\n",
        "\n",
        "print(val_acc)\n",
        "\n",
        "print(test_acc)"
      ],
      "metadata": {
        "id": "pTkTmNyzqH68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate Means"
      ],
      "metadata": {
        "id": "bfA127Yhlo5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "3YsU3jFhl3mD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define dictionary to store precision, recall, and F1-score for each class across folds\n",
        "class_metrics = defaultdict(lambda: {'precision': [], 'recall': [], 'f1-score': []})\n",
        "\n",
        "input_string = \"\"\"\n",
        "Fold 1:\n",
        "Accuracy: 90.2439024390244\n",
        "Classification report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.86      1.00      0.92         6\n",
        "           1       1.00      0.67      0.80         6\n",
        "           2       0.75      0.75      0.75         8\n",
        "           3       0.88      1.00      0.93         7\n",
        "           4       1.00      1.00      1.00        14\n",
        "\n",
        "    accuracy                           0.90        41\n",
        "   macro avg       0.90      0.88      0.88        41\n",
        "weighted avg       0.91      0.90      0.90        41\n",
        "\n",
        "\n",
        "Fold 2:\n",
        "Accuracy: 87.8048780487805\n",
        "Classification report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       1.00      1.00      1.00         7\n",
        "           1       0.75      0.86      0.80         7\n",
        "           2       0.75      0.43      0.55         7\n",
        "           3       0.78      1.00      0.88         7\n",
        "           4       1.00      1.00      1.00        13\n",
        "\n",
        "    accuracy                           0.88        41\n",
        "   macro avg       0.86      0.86      0.84        41\n",
        "weighted avg       0.88      0.88      0.87        41\n",
        "\n",
        "\n",
        "Fold 3:\n",
        "Accuracy: 90.0\n",
        "Classification report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       1.00      1.00      1.00         6\n",
        "           1       0.83      0.71      0.77         7\n",
        "           2       0.67      0.86      0.75         7\n",
        "           3       1.00      0.86      0.92         7\n",
        "           4       1.00      1.00      1.00        13\n",
        "\n",
        "    accuracy                           0.90        40\n",
        "   macro avg       0.90      0.89      0.89        40\n",
        "weighted avg       0.91      0.90      0.90        40\n",
        "\n",
        "\n",
        "Fold 4:\n",
        "Accuracy: 95.0\n",
        "Classification report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       1.00      1.00      1.00         6\n",
        "           1       0.78      1.00      0.88         7\n",
        "           2       1.00      0.71      0.83         7\n",
        "           3       1.00      1.00      1.00         7\n",
        "           4       1.00      1.00      1.00        13\n",
        "\n",
        "    accuracy                           0.95        40\n",
        "   macro avg       0.96      0.94      0.94        40\n",
        "weighted avg       0.96      0.95      0.95        40\n",
        "\n",
        "\n",
        "Fold 5:\n",
        "Accuracy: 92.5\n",
        "Classification report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       1.00      1.00      1.00         6\n",
        "           1       0.83      0.71      0.77         7\n",
        "           2       0.75      0.86      0.80         7\n",
        "           3       1.00      1.00      1.00         7\n",
        "           4       1.00      1.00      1.00        13\n",
        "\n",
        "    accuracy                           0.93        40\n",
        "   macro avg       0.92      0.91      0.91        40\n",
        "weighted avg       0.93      0.93      0.92        40\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "folds = input_string.strip().split(\"Fold\")[1:6]\n",
        "\n",
        "for report in folds:\n",
        "    lines = report.strip().split('\\n')\n",
        "    class_lines = lines[5:-4]  # Extract lines containing precision, recall, and F1-score for each class\n",
        "    for line in class_lines:\n",
        "        print(line)\n",
        "        class_id, precision, recall, f1_score, _ = line.split()\n",
        "        class_id = int(class_id)\n",
        "        precision = float(precision)\n",
        "        recall = float(recall)\n",
        "        f1_score = float(f1_score)\n",
        "        class_metrics[class_id]['precision'].append(precision)\n",
        "        class_metrics[class_id]['recall'].append(recall)\n",
        "        class_metrics[class_id]['f1-score'].append(f1_score)\n",
        "\n",
        "# Calculate mean precision, recall, and F1-score for each class\n",
        "mean_metrics = {}\n",
        "for class_id, metrics in class_metrics.items():\n",
        "    mean_precision = np.mean(metrics['precision'])\n",
        "    mean_recall = np.mean(metrics['recall'])\n",
        "    mean_f1_score = np.mean(metrics['f1-score'])\n",
        "    mean_metrics[class_id] = {'precision': mean_precision, 'recall': mean_recall, 'f1-score': mean_f1_score}\n",
        "\n",
        "# Print the results\n",
        "for class_id, metrics in mean_metrics.items():\n",
        "    print(f\"Class {class_id}:\")\n",
        "    print(f\"  Mean Precision: {metrics['precision']:.2f}\")\n",
        "    print(f\"  Mean Recall: {metrics['recall']:.2f}\")\n",
        "    print(f\"  Mean F1-score: {metrics['f1-score']:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "JtKTwWBfuNEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the results in LaTeX format\n",
        "print(\"Precision:\", \" & \".join([f\"{metrics['precision'] * 100:.0f}\" for class_id, metrics in mean_metrics.items()]), \"\\\\\\\\\")\n",
        "print(\"Recall:\", \" & \".join([f\"{metrics['recall'] * 100:.0f}\" for class_id, metrics in mean_metrics.items()]), \"\\\\\\\\\")\n",
        "print(\"F1:\", \" & \".join([f\"{metrics['f1-score'] * 100:.0f}\" for class_id, metrics in mean_metrics.items()]), \"\\\\\\\\\")"
      ],
      "metadata": {
        "id": "F6BFz_pKxiQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Install de_DE\n",
        "!/usr/share/locales/install-language-pack de_DE\n",
        "!dpkg-reconfigure locales\n",
        "\n",
        "# Restart Python process to pick up the new locales\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "aHU8sbHTs9EG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import locale\n",
        "from statsmodels.stats.proportion import proportions_ztest\n",
        "\n",
        "# Set locale to use comma as decimal point\n",
        "locale.setlocale(locale.LC_NUMERIC, \"de_DE\")\n",
        "\n",
        "# Data from the table (mean accuracy and standard deviation)\n",
        "data = {\n",
        "    'Be augmentacij≈≥': [64.32, 1.58, 88.12, 1.48, 91.11, 1.09, 89.09, 1.54],\n",
        "    'Laplasas (std/100)': [62.35, 4.00, 89.10, 1.35, 91.61, 1.08, 92.06, 1.32],\n",
        "    'Laplasas (std/50)': [61.40, 1.71, 89.61, 1.91, 91.09, 0.90, 90.57, 1.12],\n",
        "    'Laplasas (std/20)': [59.91, 1.68, 91.10, 1.11, 91.60, 0.86, 92.56, 0.72],\n",
        "    # Add data for other augmentation techniques...\n",
        "    'Dreifuojantis Gausas (std/100)': [61.89, 3.04, 91.09, 1.82, 90.10, 0.71, 91.07, 1.68],\n",
        "    'Dreifuojantis Gausas (std/50)': [64.38, 1.54, 88.10, 2.87, 92.09, 1.47, 91.57, 1.14],\n",
        "    'Dreifuojantis Gausas (std/20)': [61.85, 1.61, 92.59, 1.86, 90.12, 0.94, 89.48, 1.08],\n",
        "    'Tolygusis (std/100)': [62.87, 2.20, 89.13, 1.91, 90.07, 1.44, 91.11, 1.09],\n",
        "    'Tolygusis (std/50)': [59.93, 1.30, 89.62, 1.27, 91.09, 1.14, 90.59, 0.47],\n",
        "    'Tolygusis (std/20)': [59.94, 2.71, 89.12, 1.49, 89.09, 1.54, 89.09, 2.09],\n",
        "    'Laplasas ir Dreifuojantis Gausas': [63.39, 2.56, 92.60, 1.54, 91.07, 0.92, 90.59, 0.85],\n",
        "    'Laplasas ir Tolygusis': [62.90, 1.97, 89.11, 1.51, 91.10, 0.87, 92.57, 0.71],\n",
        "    'Dyd≈æio deformacija': [59.38, 2.13, 88.11, 1.93, 90.56, 2.19, 90.10, 1.41],\n",
        "    'TimeVAE': [75.73, 0.88, 77.17, 2.75, 90.05, 2.93, 88.57, 2.90],\n",
        "    # Add data for other augmentation techniques...\n",
        "}\n",
        "\n",
        "# Models and augmentation techniques\n",
        "models = ['CNN-LSTM', 'FCN', 'ResNet', 'InceptionTime']\n",
        "\n",
        "# Augmentation techniques\n",
        "augmentation_techniques = list(data.keys())\n",
        "\n",
        "# Base augmentation (Be augmentacij≈≥) accuracy\n",
        "base_accuracies = data['Be augmentacij≈≥'][::2]\n",
        "\n",
        "print(base_accuracies)\n",
        "\n",
        "colors = ['#ff5234', '#1f77b4', '#ff7f0e', '#2ca02c']\n",
        "\n",
        "# Plot bar plot with error bars\n",
        "plt.figure(figsize=(20, 15))  # Increase figure size\n",
        "\n",
        "plt.rcParams['axes.formatter.use_locale'] = True\n",
        "locale._override_localeconv[\"decimal_point\"]= ','\n",
        "plt.rcdefaults()\n",
        "\n",
        "# Tell matplotlib to use the locale we set above\n",
        "plt.rcParams['axes.formatter.use_locale'] = True\n",
        "\n",
        "# Width of the bars\n",
        "bar_width = 0.2\n",
        "\n",
        "# Spacing between groups\n",
        "spacing = 0.02  # Increase spacing\n",
        "\n",
        "# Position for each group\n",
        "positions = np.arange(len(augmentation_techniques))\n",
        "\n",
        "# Plot each model's accuracy\n",
        "for i, model in enumerate(models):\n",
        "    accuracies = [data[technique][i*2] if data[technique][i*2] is not None else np.nan for technique in augmentation_techniques]\n",
        "    errors = [data[technique][i*2 + 1] if data[technique][i*2] is not None else np.nan for technique in augmentation_techniques]\n",
        "    bars = plt.bar(positions + i * (bar_width + spacing), accuracies, yerr=errors, width=bar_width, label=model, color = colors[i])\n",
        "    # Add percentages on bars (vertically, centered)\n",
        "    for j, (bar, acc, err) in enumerate(zip(bars, accuracies, errors)):\n",
        "        base_acc = base_accuracies[i]\n",
        "        if not np.isnan(acc):\n",
        "            acc_str = locale.format_string(\"%.2f\", acc, grouping=True)\n",
        "            err_str = locale.format_string(\"%.2f\", err, grouping=True)\n",
        "            if acc > base_acc:\n",
        "\n",
        "                if (model == 'CNN-LSTM' and augmentation_techniques[j] == 'TimeVAE') or (model == 'FCN' and (augmentation_techniques[j] == 'Dreifuojantis Gausas (std/100)' or augmentation_techniques[j] == 'Dreifuojantis Gausas (std/20)' or augmentation_techniques[j] == 'Laplasas ir Dreifuojantis Gausas')) or (model == 'InceptionTime' and augmentation_techniques[j] == 'Laplasas ir Tolygusis'):\n",
        "                    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() - 15, f'{acc_str} ¬± -{err_str}*', ha='center', va='bottom', rotation=90, fontsize=16, weight='bold')\n",
        "                else:\n",
        "                    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() - 15, f'{acc_str} ¬± {err_str}', ha='center', va='bottom', rotation=90, fontsize=16)\n",
        "\n",
        "\n",
        "            else:\n",
        "                plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() - 15, f'{acc_str} ¬± {err_str}', ha='center', va='bottom', rotation=90, fontsize=16)\n",
        "\n",
        "\n",
        "# Set x-axis labels\n",
        "plt.xticks(positions + (len(models)) * (bar_width + spacing) / 2, augmentation_techniques, rotation=45, fontsize=16, ha='right')\n",
        "\n",
        "# Set y-axis ticks every 10%\n",
        "plt.ylim(40,100)\n",
        "plt.yticks(np.arange(40, 101, 10))\n",
        "\n",
        "# Add horizontal lines for each 10%\n",
        "for y in range(40, 101, 10):\n",
        "    plt.axhline(y=y, color='gray', linestyle='--', linewidth=0.5)\n",
        "\n",
        "# Add labels and legend\n",
        "plt.xlabel('Duomen≈≥ augmentacijos metodai', fontsize=16)\n",
        "plt.ylabel('Klasifikavimo tikslumas, %', fontsize=16)\n",
        "# plt.title('Mean Classification Accuracy with Error Bars')\n",
        "plt.legend(loc='upper right', bbox_to_anchor=(1.07, 1.04), fontsize=15)  # Move legend to upper right corner with bbox_to_anchor\n",
        "\n",
        "# Show plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JmZjyWhqoTOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data\n",
        "group1 = [90.24, 92.68, 82.5, 90.0, 90.0]\n",
        "group2 = [95.12, 92.68, 90.0, 95.0, 87.5]\n",
        "\n",
        "# conduct the Wilcoxon-Signed Rank Test\n",
        "stats.wilcoxon(group1, group2, alternative='less')"
      ],
      "metadata": {
        "id": "x3rIJoliz-iG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}